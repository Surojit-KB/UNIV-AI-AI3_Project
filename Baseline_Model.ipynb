{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surojit-KB/UNIV-AI-AI3_Project/blob/main/Baseline_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViM5hHKyG64P"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ2WxVPn06oG"
      },
      "source": [
        "#IMPORTING THE NECESSARY LIBRARIES\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Embedding, Dense, TimeDistributed,LSTM, GRU,Dropout, Bidirectional, Conv1D, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmc7WcDt_gx4",
        "outputId": "a861f190-f469-45c7-8e6d-05b624feb477"
      },
      "source": [
        "#INSTALLING THE SENTENCEPIECE LIBRARY FOR SENTENCEPIECE TOKENIZER\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 33.3MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 35.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 33.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 35.9MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 26.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 27.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 28.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 25.4MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 25.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 25.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 25.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 25.4MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 25.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 25.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 25.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kji07lM1QdY",
        "outputId": "80266757-1fee-40c5-ba8f-b380082304b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X94_u0H2PYFa"
      },
      "source": [
        "## 2. Getting the data and preprocessing\n",
        "The SQUAD dataset is in the form of a JSON file, hence we need to load it manually into a dataframe. This part of the notebook focuses on loading the dataframe, preprocessing the data and getting the context text ready for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-sJeZUHIzc_"
      },
      "source": [
        "#THIS IS THE CODE TO PREPROCESS THE SQUAD DATASET AND CREATE A DATAFRAME OUT OF IT\n",
        "def get_dataframe(file):\n",
        "  f = open(file, 'r')\n",
        "  #loading json file \n",
        "  data = json.loads(f.read())\n",
        "  #creating empty lists to store df values \n",
        "  iid = []\n",
        "  tit = []\n",
        "  con = []\n",
        "  que = []\n",
        "  ans = []\n",
        "  txt = []\n",
        "  #root tags contains 'title' tag and 'paragraphs' list \n",
        "  for i in range(len(data['data'])):\n",
        "    title = data['data'][i]['title']\n",
        "    #'paragraphs' list contains 'context' tag and 'qas' list \n",
        "    for p in range(len(data['data'][i]['paragraphs'])):\n",
        "      context = data['data'][i]['paragraphs'][p]['context']\n",
        "      for q in range(len(data['data'][i]['paragraphs'][p]['qas'])):\n",
        "        # 'qas'list contains 'question', 'Id' tag and 'answers' list \n",
        "        question = data['data'][i]['paragraphs'][p]['qas'][q]['question']\n",
        "        id = data['data'][i]['paragraphs'][p]['qas'][q]['id']\n",
        "        #'answers' list contains 'answer_start' and 'text' tag \n",
        "        for a in range(len(data['data'][i]['paragraphs'][p]['qas'][q]['answers'])):\n",
        "          ans_start = data['data'][i]['paragraphs'][p]['qas'][q]['answers'][a]['answer_start']\n",
        "          text = data['data'][i]['paragraphs'][p]['qas'][q]['answers'][a]['text']\n",
        "          \n",
        "          #appending values to list \n",
        "          iid.append(id)\n",
        "          tit.append(title)\n",
        "          con.append(context)\n",
        "          que.append(question)\n",
        "          ans.append(ans_start)\n",
        "          txt.append(text)\n",
        "  #creating dataframe from lists \n",
        "  new_df = pd.DataFrame(columns=['Id', 'title', 'context', 'question', 'ans_start', 'text'])\n",
        "  new_df.Id = iid\n",
        "  new_df.title=tit\n",
        "  new_df.context = con\n",
        "  new_df.question = que\n",
        "  new_df.ans_start = ans \n",
        "  new_df.text = txt \n",
        "  #removing duplicate columns \n",
        "  final_df = new_df.drop_duplicates(keep='first')\n",
        "\n",
        "  return final_df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVjCaABR1QbA"
      },
      "source": [
        "#GETTING THE DATAFRAME READY\n",
        "df=get_dataframe('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/train-v2.0.json')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "lILTaiexBCHh",
        "outputId": "7272533f-ebf8-431d-af35-2fdf5855abdf"
      },
      "source": [
        "#DISPLAY THE DATAFRAME\n",
        "df.head(10)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>ans_start</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>269</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>207</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>526</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>166</td>\n",
              "      <td>Houston, Texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>276</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9603</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In what R&amp;B group was she the lead singer?</td>\n",
              "      <td>320</td>\n",
              "      <td>Destiny's Child</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9604</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What album made her a worldwide known artist?</td>\n",
              "      <td>505</td>\n",
              "      <td>Dangerously in Love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9605</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Who managed the Destiny's Child group?</td>\n",
              "      <td>360</td>\n",
              "      <td>Mathew Knowles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>56d43c5f2ccc5a1400d830a9</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyoncé rise to fame?</td>\n",
              "      <td>276</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>56d43c5f2ccc5a1400d830aa</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What role did Beyoncé have in Destiny's Child?</td>\n",
              "      <td>290</td>\n",
              "      <td>lead singer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Id    title  ... ans_start                 text\n",
              "0  56be85543aeaaa14008c9063  Beyoncé  ...       269    in the late 1990s\n",
              "1  56be85543aeaaa14008c9065  Beyoncé  ...       207  singing and dancing\n",
              "2  56be85543aeaaa14008c9066  Beyoncé  ...       526                 2003\n",
              "3  56bf6b0f3aeaaa14008c9601  Beyoncé  ...       166       Houston, Texas\n",
              "4  56bf6b0f3aeaaa14008c9602  Beyoncé  ...       276           late 1990s\n",
              "5  56bf6b0f3aeaaa14008c9603  Beyoncé  ...       320      Destiny's Child\n",
              "6  56bf6b0f3aeaaa14008c9604  Beyoncé  ...       505  Dangerously in Love\n",
              "7  56bf6b0f3aeaaa14008c9605  Beyoncé  ...       360       Mathew Knowles\n",
              "8  56d43c5f2ccc5a1400d830a9  Beyoncé  ...       276           late 1990s\n",
              "9  56d43c5f2ccc5a1400d830aa  Beyoncé  ...       290          lead singer\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1cij-7iMc2t",
        "outputId": "7663efb2-4ac7-4702-ed5f-39ef7d569122"
      },
      "source": [
        "#INFO ABOUT THE DATASET\n",
        "print('The number of different titles in the SQUAD dataset are:',df.title.nunique())\n",
        "print('The number of different context paragraphs in the SQUAD dataset are:',df.context.nunique())\n",
        "print('The number of unique question-answer pairs in the SQUAD dataset are:',df.question.nunique())"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of different titles in the SQUAD dataset are: 440\n",
            "The number of different context paragraphs in the SQUAD dataset are: 18877\n",
            "The number of unique question-answer pairs in the SQUAD dataset are: 86769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg2GLiuNQ6-o"
      },
      "source": [
        "#NEXT WE WILL CREATE A TEXT STRING OUT OF ALL THE UNIQUE CONTEXT PARAGRAPHS WE HAVE\n",
        "#THIS TEXT WILL BE USED TO TRAIN A SIMPLE BASELINE MODEL\n",
        "text=''.join(df.context.unique())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wpzdhSpJCD6"
      },
      "source": [
        "#SINCE THE DATA CONTAINS A LOT OF PHONETIC CHARACTERS, WE WILL CONVERT THEM TO NORMAL CHARACTERS FOR SAKE OF BETTER ENCODING\n",
        "#THIS WILL HELP US IN BETTER TOKENIZATION\n",
        "import unicodedata\n",
        "normalized_text=str(unicodedata.normalize('NFD', text).encode('ascii', 'ignore'))\n",
        "#FURTHER CLEANING TO REMOVE THE SPECIAL CHARACTERS AND LOWERCASE THE TEXT\n",
        "cleaned_text=re.sub('[^A-Za-z0-9.\\' ]+','',normalized_text[2:])\n",
        "cleaned_text=cleaned_text.lower()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHxhlie2Vl68",
        "outputId": "7c3bbc80-1a7a-48da-82dd-2971abcd22b1"
      },
      "source": [
        "#TO CONVERT THE TEXT INTO SENTENCES, WE ARE USING A TOKENIZER FROM THE NLTK LIBRARY.\n",
        "#THE TEXT THAT WE HAVE HAS QUITE MANY PERIODS WHICH DO NOT ALWAYS DENOTE THE END OF A SENTENCE\n",
        "#NLTK TOKENIZER IS BEST SUITED FOR THIS PURPOSE\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "list_of_sentence=tokenizer.tokenize(cleaned_text)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HfTBSgMNZBL",
        "outputId": "55864c69-62e9-4c50-fa58-9d5870600822"
      },
      "source": [
        "#EXAMPLE OUTPUT OF THE TOKENIZER FOR CREATING A LIST OF SENTENCE\n",
        "list_of_sentence[:10]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['beyonce giselle knowlescarter bijnse beeyonsay born september 4 1981 is an american singer songwriter record producer and actress.',\n",
              " \"born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of rb girlgroup destiny's child.\",\n",
              " \"managed by her father mathew knowles the group became one of the world's bestselling girl groups of all time.\",\n",
              " \"their hiatus saw the release of beyonce's debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 numberone singles crazy in love and baby boy.following the disbandment of destiny's child in june 2005 she released her second solo album b'day 2006 which contained hits deja vu irreplaceable and beautiful liar.\",\n",
              " 'beyonce also ventured into acting with a golden globenominated performance in dreamgirls 2006 and starring roles in the pink panther 2006 and obsessed 2009. her marriage to rapper jay z and portrayal of etta james in cadillac records 2008 influenced her third album i am... sasha fierce 2008 which saw the birth of her alterego sasha fierce and earned a recordsetting six grammy awards in 2010 including song of the year for single ladies put a ring on it.',\n",
              " 'beyonce took a hiatus from music in 2010 and took over management of her career her fourth album 4 2011 was subsequently mellower in tone exploring 1970s funk 1980s pop and 1990s soul.',\n",
              " 'her critically acclaimed fifth studio album beyonce 2013 was distinguished from previous releases by its experimental production and exploration of darker themes.a selfdescribed modernday feminist beyonce creates songs that are often characterized by themes of love relationships and monogamy as well as female sexuality and empowerment.',\n",
              " 'on stage her dynamic highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music.',\n",
              " \"throughout a career spanning 19 years she has sold over 118 million records as a solo artist and a further 60 million with destiny's child making her one of the bestselling music artists of all time.\",\n",
              " \"she has won 20 grammy awards and is the most nominated woman in the award's history.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQQlBebqP-8l"
      },
      "source": [
        "## 3. TOKENIZATION\n",
        "Next we will tokenize the context data using the SentencePiece Tokenizer. \n",
        "\n",
        "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing. More info can be found [here](https://github.com/google/sentencepiece).\n",
        "\n",
        "We will use a tokenizer with byte-pair-encoding.\n",
        "The general idea of byte-pair-encoding is to do the following ([source](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)):\n",
        "\n",
        "1. Go through your corpus and find all the “bytes” i.e. the irreducible characters from which all others can be built. This is our base. It ensures we can almost always reconstruct any unseen input.\n",
        "2. Run a sliding window over the entire corpus (the actual code is slightly different) and find the most frequent bigram. Bigrams are formed from consecutive subwords in the current list of seen subwords. So, “hello” would have the counts {“he”: 1, “el”:1, “ll”:1, “lo”: 1}.\n",
        "3. Choose the most frequent bigram, add it to the list of subwords, then merge all instances of this bigram in the corpus.\n",
        "4. Repeat until you reach your desired vocabulary size.\n",
        "\n",
        "The vocabulary size is fixed at 6000 for the complete baseline modelling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoFkXlsY__v3"
      },
      "source": [
        "#WE WILL SAVE ALL THE SENTENCES IN A TEXT FILE, WHICH WILL BE PROVIDED THE TOKENIZER TO WORK ON\n",
        "rect_path='./context.txt'\n",
        "with open(rect_path, 'w', errors = 'ignore') as file:\n",
        "    for line in list_of_sentence:\n",
        "      file.write(line+'\\n')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDs0AJn--cX"
      },
      "source": [
        "#DEFINING THE TOKENIZER\n",
        "def sentencepiece_():\n",
        "  templates= '--input={} \\\n",
        "  --pad_id={} \\\n",
        "  --bos_id={} \\\n",
        "  --eos_id={} \\\n",
        "  --unk_id={} \\\n",
        "  --model_prefix={} \\\n",
        "  --vocab_size={} \\\n",
        "  --character_coverage={} \\\n",
        "  --model_type={}'\n",
        "\n",
        "  train_input_file = rect_path\n",
        "  prefix = './en_spm' \n",
        "  pad_id=0  #PAD ID\n",
        "  vocab_size = 6000 \n",
        "  bos_id=1  #BEGIN OF SENTENCE ID\n",
        "  eos_id=2  #END OF SENTENCE ID\n",
        "  unk_id=3  #UNKNOWN ID\n",
        "  character_coverage = 1.0 \n",
        "  model_type ='bpe' \n",
        "\n",
        "\n",
        "  cmd = templates.format(train_input_file,\n",
        "                pad_id,\n",
        "                bos_id,\n",
        "                eos_id,\n",
        "                unk_id,\n",
        "                prefix,\n",
        "                vocab_size,\n",
        "                character_coverage,\n",
        "                model_type)\n",
        " \n",
        "  spm.SentencePieceTrainer.train(cmd) #TRAINING THE TOKENIZER\n",
        "\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  sp.load('./en_spm.model')\n",
        "  return sp"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJsV7uAnApIt"
      },
      "source": [
        "sp=sentencepiece_() #TRAINED TOKENIZER"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXwXLZdo-L6q"
      },
      "source": [
        "#NEXT WE GET THE VOCABULARY OUT OF THE TOKENIZER MODEL\n",
        "#WE WILL CREATE A WORD TO INDEX AND INDEX TO WORD DICTIONARY FOR FURTHER USE\n",
        "def dictionary():\n",
        "  with open('./en_spm.vocab', encoding='utf-8') as f:\n",
        "      Vo = [doc.strip().split(\"\\t\") for doc in f]\n",
        "\n",
        "  # w[0]: token name    \n",
        "  # w[1]: token score \n",
        "  word2idx_en = {w[0]: i for i, w in enumerate(Vo)}\n",
        "  idx2word_en = {i:w[0] for i, w in enumerate(Vo)}\n",
        "\n",
        "  return word2idx_en, idx2word_en\n",
        "word2idx,idx2word=dictionary()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5jGS5m-CcE7"
      },
      "source": [
        "#NOW, WE CREATE FUNCTIONS FOR ENCODING THE INPUT TEXT AND CONVERTING THEM TO SEQUENCES\n",
        "#ENCODING WILL SEPARATE THE WORDS IN THE TEXT INTO SUB-WORDS AS PER THE TOKENIZER VOCAB\n",
        "#THEN WE WILL ENCODE THESE SUB-WORDS INTO TOKENS BASED ON VOCAB\n",
        "def encode_data(input):\n",
        "            pieces = sp.encode_as_pieces(input)\n",
        "            return pieces\n",
        "def texts_to_sequences(texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            text_list = text.split()\n",
        "            sequence = [int(word2idx.get(token, word2idx[\"<unk>\"])) for token in text_list]\n",
        "            sequences += sequence\n",
        "        return sequences"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFf5UNv7C1Vl"
      },
      "source": [
        "#HERE WE CREATE THE TOKEN LIST FOR ALL THE SENTENCES WE HAVE\n",
        "token_list=[]\n",
        "for i in list_of_sentence:\n",
        "  encoded_data=encode_data(i)\n",
        "  tokens=texts_to_sequences(encoded_data)\n",
        "  tokens.append(2) #ADDING THE END TOKEN TO THE TOKEN-LIST\n",
        "  tokens=[1]+tokens #ADDING THE START TOKEN TO THE TOKEN-LIST\n",
        "  token_list.append(tokens)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_aJE8vmug-_",
        "outputId": "053a600e-c17a-427a-c141-78e3c1665d66"
      },
      "source": [
        "print('The MIN sequence length of the input data is:',np.min([len(i) for i in token_list]))\n",
        "print('The MAX sequence length of the input data is:',np.max([len(i) for i in token_list]))\n",
        "print('The mean sequence length of the input data is:',np.mean([len(i) for i in token_list]))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The MIN sequence length of the input data is: 3\n",
            "The MAX sequence length of the input data is: 738\n",
            "The mean sequence length of the input data is: 48.01034322009207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T32Qa4mLMjyY"
      },
      "source": [
        "#TO MAKE SURE WE HAVE SUFFICIENT DATA FOR TRAINING WITHOUT HAVING TOO MUCH PADDED SEQUENCES,\n",
        "#THE MINIMUM SEQUENCE LENTGH WAS CONSIDERED TO BE 20 AND THE MAX SEQUENCE LENGTH WAS CONSIDERED TO BE 250\n",
        "cleaned_list=[]\n",
        "for i in token_list:\n",
        "  if len(i)>20:\n",
        "    if len(i)>250:\n",
        "      cleaned_list.append(i[:250]+[2])\n",
        "    else:\n",
        "      cleaned_list.append(i)\n",
        "maxlen=len(max(cleaned_list,key=len))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcT9VhxjvAXo",
        "outputId": "c027d350-c6b8-49ff-9f6e-e6082b51000d"
      },
      "source": [
        "print('The maximum sequence length of the input data is:',maxlen)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The maximum sequence length of the input data is: 251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZwdLtslvOi7",
        "outputId": "48b7ba72-471f-4b6f-8c52-d7720cdd40cb"
      },
      "source": [
        "#LETS SEE WHAT AN INPUT SENTENCE LOOKS LIKE IN TERMS OF THE ENCODING\n",
        "print([idx2word[i] for i in token_list[8]])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', '▁throughout', '▁a', '▁career', '▁sp', 'an', 'ning', '▁19', '▁years', '▁she', '▁has', '▁sold', '▁over', '▁11', '8', '▁million', '▁records', '▁as', '▁a', '▁sol', 'o', '▁artist', '▁and', '▁a', '▁further', '▁60', '▁million', '▁with', '▁dest', 'iny', \"'\", 's', '▁child', '▁making', '▁her', '▁one', '▁of', '▁the', '▁best', 'se', 'lling', '▁music', '▁artists', '▁of', '▁all', '▁time', '.', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do_24FwWVwDH"
      },
      "source": [
        "## 4. MODELLING THE BASELINE LANGUAGE MODEL\n",
        "Now we will creata a baseline language model which will be trained on the context data and provide us with the embeddings for the vocabulary. It is a very basic Sequential model consisting of:\n",
        "1. An Embedding Layer of dimension 300\n",
        "2. An LSTM layer of dimension 512\n",
        "3. An Dropout Layer with dropout rate of 0.2\n",
        "4. A second LSTM layer of dimension 512\n",
        "5. A second Dropout Layer with dropout rate of 0.2\n",
        "6. A Dense layer of vocab size with softmax activation\n",
        "\n",
        "The batch size of the dataset is 128, the model was trained for around 40 epochs in 4 rounds (since it took a long time to train the epochs, the model was trained for 10 epochs in each round and the weights were saved after each round)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQC4XYBw1QQD"
      },
      "source": [
        "#Function for creating the dataset\n",
        "def get_dataset(token_list):\n",
        "  inp_token=[i[:-1] for i in token_list]\n",
        "  target_token=[i[1:] for i in token_list]\n",
        "  ragged_inp=tf.ragged.constant(inp_token)\n",
        "  ragged_target=tf.ragged.constant(target_token)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((ragged_inp,ragged_target))\n",
        "  dataset = dataset.batch(128)\n",
        "  dataset = dataset.map(lambda x,y: (x.to_tensor(default_value=0, shape=[None, None]), y.to_tensor(default_value=0, shape=[None, None])), num_parallel_calls=5)\n",
        "  dataset = dataset.prefetch(1)\n",
        "  return dataset\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huwTY3vXNWuf"
      },
      "source": [
        "dataset=get_dataset(cleaned_list)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7vIGyclQF4x",
        "outputId": "4a953da1-f4ab-4dc3-f81d-9836b3166685"
      },
      "source": [
        "for batch in dataset:\n",
        "  print(batch[0],batch[1])\n",
        "  break"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   1 3818   49 ...    0    0    0]\n",
            " [   1 3107   34 ...    0    0    0]\n",
            " [   1 4610   93 ...    0    0    0]\n",
            " ...\n",
            " [   1    8 1727 ...    0    0    0]\n",
            " [   1 2692 4675 ...    0    0    0]\n",
            " [   1  130  410 ...    0    0    0]], shape=(128, 147), dtype=int32) tf.Tensor(\n",
            "[[3818   49   27 ...    0    0    0]\n",
            " [3107   34 3882 ...    0    0    0]\n",
            " [4610   93  576 ...    0    0    0]\n",
            " ...\n",
            " [   8 1727 3501 ...    0    0    0]\n",
            " [2692 4675 5979 ...    0    0    0]\n",
            " [ 130  410 1746 ...    0    0    0]], shape=(128, 147), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MLTf_D4NWl7"
      },
      "source": [
        "def create_model():\n",
        "  embed_dim=300\n",
        "  vocab_size=6000\n",
        "  model=Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,mask_zero=True))\n",
        "  model.add(LSTM(units=512,return_sequences=True))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "  model.add(LSTM(units=512,return_sequences=True))\n",
        "  model.add(Dropout(rate=0.1))\n",
        "  model.add(TimeDistributed(Dense(units=vocab_size,activation='softmax')))\n",
        "  model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics='accuracy')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY0pccdJVjOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44b4fe0-8c06-4d82-95db-c5d131d3d793"
      },
      "source": [
        "model=create_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 300)         1800000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 512)         1665024   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 512)         2099200   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, None, 6000)        3078000   \n",
            "=================================================================\n",
            "Total params: 8,642,224\n",
            "Trainable params: 8,642,224\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d0uM20GiXVtQ",
        "outputId": "9817ef38-1770-4946-d6a9-e079482204c5"
      },
      "source": [
        "import pydot\n",
        "from tensorflow.keras.utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "SVG(model_to_dot(model2,  show_shapes=True, show_layer_names=True, dpi=52, rankdir='TB').create_svg())"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"1179pt\" viewBox=\"0.00 0.00 2012.00 1632.00\" width=\"1453pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.7222 .7222) rotate(0) translate(4 1628)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-1628 2008,-1628 2008,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139900642431824 -->\n<g class=\"node\" id=\"node1\">\n<title>139900642431824</title>\n<polygon fill=\"none\" points=\"248.5,-1577.5 248.5,-1623.5 544.5,-1623.5 544.5,-1577.5 248.5,-1577.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-1596.8\">input_1: InputLayer</text>\n<polyline fill=\"none\" points=\"381.5,-1577.5 381.5,-1623.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"410.5\" y=\"-1608.3\">input:</text>\n<polyline fill=\"none\" points=\"381.5,-1600.5 439.5,-1600.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"410.5\" y=\"-1585.3\">output:</text>\n<polyline fill=\"none\" points=\"439.5,-1577.5 439.5,-1623.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"492\" y=\"-1608.3\">[(None, None)]</text>\n<polyline fill=\"none\" points=\"439.5,-1600.5 544.5,-1600.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"492\" y=\"-1585.3\">[(None, None)]</text>\n</g>\n<!-- 139900642432784 -->\n<g class=\"node\" id=\"node2\">\n<title>139900642432784</title>\n<polygon fill=\"none\" points=\"397.5,-1494.5 397.5,-1540.5 793.5,-1540.5 793.5,-1494.5 397.5,-1494.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"503.5\" y=\"-1513.8\">Encoder_embedding: Embedding</text>\n<polyline fill=\"none\" points=\"609.5,-1494.5 609.5,-1540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"638.5\" y=\"-1525.3\">input:</text>\n<polyline fill=\"none\" points=\"609.5,-1517.5 667.5,-1517.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"638.5\" y=\"-1502.3\">output:</text>\n<polyline fill=\"none\" points=\"667.5,-1494.5 667.5,-1540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730.5\" y=\"-1525.3\">(None, None)</text>\n<polyline fill=\"none\" points=\"667.5,-1517.5 793.5,-1517.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730.5\" y=\"-1502.3\">(None, None, 300)</text>\n</g>\n<!-- 139900642431824&#45;&gt;139900642432784 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139900642431824-&gt;139900642432784</title>\n<path d=\"M451.6683,-1577.4901C476.255,-1567.2353 505.3812,-1555.0872 530.9713,-1544.414\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"532.3423,-1547.6345 540.2244,-1540.5547 529.6476,-1541.1739 532.3423,-1547.6345\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901717264 -->\n<g class=\"node\" id=\"node20\">\n<title>139901901717264</title>\n<polygon fill=\"none\" points=\"20.5,-1494.5 20.5,-1540.5 378.5,-1540.5 378.5,-1494.5 20.5,-1494.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"122.5\" y=\"-1513.8\">tf.math.greater_1: TFOpLambda</text>\n<polyline fill=\"none\" points=\"224.5,-1494.5 224.5,-1540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253.5\" y=\"-1525.3\">input:</text>\n<polyline fill=\"none\" points=\"224.5,-1517.5 282.5,-1517.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253.5\" y=\"-1502.3\">output:</text>\n<polyline fill=\"none\" points=\"282.5,-1494.5 282.5,-1540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-1525.3\">(None, None)</text>\n<polyline fill=\"none\" points=\"282.5,-1517.5 378.5,-1517.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-1502.3\">(None, None)</text>\n</g>\n<!-- 139900642431824&#45;&gt;139901901717264 -->\n<g class=\"edge\" id=\"edge19\">\n<title>139900642431824-&gt;139901901717264</title>\n<path d=\"M341.8861,-1577.4901C317.6533,-1567.2803 288.9659,-1555.1938 263.7137,-1544.5545\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"264.7945,-1541.212 254.2201,-1540.5547 262.0766,-1547.6628 264.7945,-1541.212\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642434576 -->\n<g class=\"node\" id=\"node4\">\n<title>139900642434576</title>\n<polygon fill=\"none\" points=\"427,-1411.5 427,-1457.5 764,-1457.5 764,-1411.5 427,-1411.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"503.5\" y=\"-1430.8\">Encoder_RNN_f: GRU</text>\n<polyline fill=\"none\" points=\"580,-1411.5 580,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"609\" y=\"-1442.3\">input:</text>\n<polyline fill=\"none\" points=\"580,-1434.5 638,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"609\" y=\"-1419.3\">output:</text>\n<polyline fill=\"none\" points=\"638,-1411.5 638,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"701\" y=\"-1442.3\">(None, None, 300)</text>\n<polyline fill=\"none\" points=\"638,-1434.5 764,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"701\" y=\"-1419.3\">(None, None, 256)</text>\n</g>\n<!-- 139900642432784&#45;&gt;139900642434576 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139900642432784-&gt;139900642434576</title>\n<path d=\"M595.5,-1494.3799C595.5,-1486.1745 595.5,-1476.7679 595.5,-1467.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"599.0001,-1467.784 595.5,-1457.784 592.0001,-1467.784 599.0001,-1467.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642432336 -->\n<g class=\"node\" id=\"node3\">\n<title>139900642432336</title>\n<polygon fill=\"none\" points=\"1467.5,-1411.5 1467.5,-1457.5 1763.5,-1457.5 1763.5,-1411.5 1467.5,-1411.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1534\" y=\"-1430.8\">input_2: InputLayer</text>\n<polyline fill=\"none\" points=\"1600.5,-1411.5 1600.5,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1629.5\" y=\"-1442.3\">input:</text>\n<polyline fill=\"none\" points=\"1600.5,-1434.5 1658.5,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1629.5\" y=\"-1419.3\">output:</text>\n<polyline fill=\"none\" points=\"1658.5,-1411.5 1658.5,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1711\" y=\"-1442.3\">[(None, None)]</text>\n<polyline fill=\"none\" points=\"1658.5,-1434.5 1763.5,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1711\" y=\"-1419.3\">[(None, None)]</text>\n</g>\n<!-- 139901901774224 -->\n<g class=\"node\" id=\"node5\">\n<title>139901901774224</title>\n<polygon fill=\"none\" points=\"1223.5,-1328.5 1223.5,-1374.5 1619.5,-1374.5 1619.5,-1328.5 1223.5,-1328.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1329.5\" y=\"-1347.8\">Decoder_embedding: Embedding</text>\n<polyline fill=\"none\" points=\"1435.5,-1328.5 1435.5,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1464.5\" y=\"-1359.3\">input:</text>\n<polyline fill=\"none\" points=\"1435.5,-1351.5 1493.5,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1464.5\" y=\"-1336.3\">output:</text>\n<polyline fill=\"none\" points=\"1493.5,-1328.5 1493.5,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1556.5\" y=\"-1359.3\">(None, None)</text>\n<polyline fill=\"none\" points=\"1493.5,-1351.5 1619.5,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1556.5\" y=\"-1336.3\">(None, None, 300)</text>\n</g>\n<!-- 139900642432336&#45;&gt;139901901774224 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139900642432336-&gt;139901901774224</title>\n<path d=\"M1561.7178,-1411.4901C1537.854,-1401.2803 1509.6035,-1389.1938 1484.7358,-1378.5545\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1485.9575,-1375.2703 1475.3868,-1374.5547 1483.204,-1381.7061 1485.9575,-1375.2703\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642437968 -->\n<g class=\"node\" id=\"node13\">\n<title>139900642437968</title>\n<polygon fill=\"none\" points=\"1638,-1328.5 1638,-1374.5 1981,-1374.5 1981,-1328.5 1638,-1328.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1732.5\" y=\"-1347.8\">tf.math.greater: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1827,-1328.5 1827,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1856\" y=\"-1359.3\">input:</text>\n<polyline fill=\"none\" points=\"1827,-1351.5 1885,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1856\" y=\"-1336.3\">output:</text>\n<polyline fill=\"none\" points=\"1885,-1328.5 1885,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1933\" y=\"-1359.3\">(None, None)</text>\n<polyline fill=\"none\" points=\"1885,-1351.5 1981,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1933\" y=\"-1336.3\">(None, None)</text>\n</g>\n<!-- 139900642432336&#45;&gt;139900642437968 -->\n<g class=\"edge\" id=\"edge12\">\n<title>139900642432336-&gt;139900642437968</title>\n<path d=\"M1669.2822,-1411.4901C1693.146,-1401.2803 1721.3965,-1389.1938 1746.2642,-1378.5545\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1747.796,-1381.7061 1755.6132,-1374.5547 1745.0425,-1375.2703 1747.796,-1381.7061\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901712720 -->\n<g class=\"node\" id=\"node6\">\n<title>139901901712720</title>\n<polygon fill=\"none\" points=\"751.5,-1328.5 751.5,-1374.5 1205.5,-1374.5 1205.5,-1328.5 751.5,-1328.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"886.5\" y=\"-1347.8\">tf.__operators__.getitem: SlicingOpLambda</text>\n<polyline fill=\"none\" points=\"1021.5,-1328.5 1021.5,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1050.5\" y=\"-1359.3\">input:</text>\n<polyline fill=\"none\" points=\"1021.5,-1351.5 1079.5,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1050.5\" y=\"-1336.3\">output:</text>\n<polyline fill=\"none\" points=\"1079.5,-1328.5 1079.5,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1142.5\" y=\"-1359.3\">(None, None, 256)</text>\n<polyline fill=\"none\" points=\"1079.5,-1351.5 1205.5,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1142.5\" y=\"-1336.3\">(None, 256)</text>\n</g>\n<!-- 139900642434576&#45;&gt;139901901712720 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139900642434576-&gt;139901901712720</title>\n<path d=\"M701.6782,-1411.4901C751.4889,-1400.6956 810.9802,-1387.8033 862.0244,-1376.7415\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"863.0834,-1380.0933 872.1152,-1374.5547 861.6008,-1373.2521 863.0834,-1380.0933\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642431760 -->\n<g class=\"node\" id=\"node9\">\n<title>139900642431760</title>\n<polygon fill=\"none\" points=\"457.5,-1328.5 457.5,-1374.5 733.5,-1374.5 733.5,-1328.5 457.5,-1328.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"503.5\" y=\"-1347.8\">dense: Dense</text>\n<polyline fill=\"none\" points=\"549.5,-1328.5 549.5,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"578.5\" y=\"-1359.3\">input:</text>\n<polyline fill=\"none\" points=\"549.5,-1351.5 607.5,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"578.5\" y=\"-1336.3\">output:</text>\n<polyline fill=\"none\" points=\"607.5,-1328.5 607.5,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"670.5\" y=\"-1359.3\">(None, None, 256)</text>\n<polyline fill=\"none\" points=\"607.5,-1351.5 733.5,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"670.5\" y=\"-1336.3\">(None, None, 64)</text>\n</g>\n<!-- 139900642434576&#45;&gt;139900642431760 -->\n<g class=\"edge\" id=\"edge8\">\n<title>139900642434576-&gt;139900642431760</title>\n<path d=\"M595.5,-1411.3799C595.5,-1403.1745 595.5,-1393.7679 595.5,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"599.0001,-1384.784 595.5,-1374.784 592.0001,-1384.784 599.0001,-1384.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139902139113040 -->\n<g class=\"node\" id=\"node7\">\n<title>139902139113040</title>\n<polygon fill=\"none\" points=\"903.5,-1245.5 903.5,-1291.5 1315.5,-1291.5 1315.5,-1245.5 903.5,-1245.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"974\" y=\"-1264.8\">Decoder_RNN: GRU</text>\n<polyline fill=\"none\" points=\"1044.5,-1245.5 1044.5,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1073.5\" y=\"-1276.3\">input:</text>\n<polyline fill=\"none\" points=\"1044.5,-1268.5 1102.5,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1073.5\" y=\"-1253.3\">output:</text>\n<polyline fill=\"none\" points=\"1102.5,-1245.5 1102.5,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1209\" y=\"-1276.3\">[(None, None, 300), (None, 256)]</text>\n<polyline fill=\"none\" points=\"1102.5,-1268.5 1315.5,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1209\" y=\"-1253.3\">(None, None, 256)</text>\n</g>\n<!-- 139901901774224&#45;&gt;139902139113040 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139901901774224-&gt;139902139113040</title>\n<path d=\"M1335.0049,-1328.4901C1294.9353,-1317.8306 1247.1756,-1305.1252 1205.946,-1294.1571\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1206.727,-1290.7432 1196.1633,-1291.5547 1204.9274,-1297.5079 1206.727,-1290.7432\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901712720&#45;&gt;139902139113040 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139901901712720-&gt;139902139113040</title>\n<path d=\"M1014.9908,-1328.3799C1030.1937,-1318.7475 1048.0094,-1307.4597 1064.0498,-1297.2967\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1066.1765,-1300.0926 1072.7505,-1291.784 1062.4301,-1294.1796 1066.1765,-1300.0926\" stroke=\"#000000\"/>\n</g>\n<!-- 139902076319312 -->\n<g class=\"node\" id=\"node8\">\n<title>139902076319312</title>\n<polygon fill=\"none\" points=\"1051,-1162.5 1051,-1208.5 1342,-1208.5 1342,-1162.5 1051,-1162.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1104.5\" y=\"-1181.8\">dense_1: Dense</text>\n<polyline fill=\"none\" points=\"1158,-1162.5 1158,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1187\" y=\"-1193.3\">input:</text>\n<polyline fill=\"none\" points=\"1158,-1185.5 1216,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1187\" y=\"-1170.3\">output:</text>\n<polyline fill=\"none\" points=\"1216,-1162.5 1216,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1279\" y=\"-1193.3\">(None, None, 256)</text>\n<polyline fill=\"none\" points=\"1216,-1185.5 1342,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1279\" y=\"-1170.3\">(None, None, 64)</text>\n</g>\n<!-- 139902139113040&#45;&gt;139902076319312 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139902139113040-&gt;139902076319312</title>\n<path d=\"M1133.7344,-1245.3799C1143.3635,-1236.1935 1154.5709,-1225.5013 1164.8286,-1215.7152\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1167.2744,-1218.2192 1172.0939,-1208.784 1162.4424,-1213.1544 1167.2744,-1218.2192\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642491984 -->\n<g class=\"node\" id=\"node31\">\n<title>139900642491984</title>\n<polygon fill=\"none\" points=\"876.5,-83.5 876.5,-129.5 1346.5,-129.5 1346.5,-83.5 876.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"956.5\" y=\"-102.8\">concatenate: Concatenate</text>\n<polyline fill=\"none\" points=\"1036.5,-83.5 1036.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1065.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"1036.5,-106.5 1094.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1065.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"1094.5,-83.5 1094.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1220.5\" y=\"-114.3\">[(None, None, 256), (None, None, 256)]</text>\n<polyline fill=\"none\" points=\"1094.5,-106.5 1346.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1220.5\" y=\"-91.3\">(None, None, 512)</text>\n</g>\n<!-- 139902139113040&#45;&gt;139900642491984 -->\n<g class=\"edge\" id=\"edge31\">\n<title>139902139113040-&gt;139900642491984</title>\n<path d=\"M1077.5786,-1245.3194C1065.3712,-1235.2299 1052.0926,-1222.6035 1042.5,-1209 1013.0829,-1167.2829 999.5,-1153.5459 999.5,-1102.5 999.5,-1102.5 999.5,-1102.5 999.5,-272.5 999.5,-222.9464 1003.9957,-206.5347 1032.5,-166 1040.6066,-154.472 1051.6358,-144.2198 1062.8802,-135.6104\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1065.0518,-138.3585 1071.0672,-129.6369 1060.9259,-132.7037 1065.0518,-138.3585\" stroke=\"#000000\"/>\n</g>\n<!-- 139902076321744 -->\n<g class=\"node\" id=\"node10\">\n<title>139902076321744</title>\n<polygon fill=\"none\" points=\"1040.5,-1079.5 1040.5,-1125.5 1428.5,-1125.5 1428.5,-1079.5 1040.5,-1079.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1138.5\" y=\"-1098.8\">tf.expand_dims: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1236.5,-1079.5 1236.5,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1265.5\" y=\"-1110.3\">input:</text>\n<polyline fill=\"none\" points=\"1236.5,-1102.5 1294.5,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1265.5\" y=\"-1087.3\">output:</text>\n<polyline fill=\"none\" points=\"1294.5,-1079.5 1294.5,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1361.5\" y=\"-1110.3\">(None, None, 64)</text>\n<polyline fill=\"none\" points=\"1294.5,-1102.5 1428.5,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1361.5\" y=\"-1087.3\">(None, None, 1, 64)</text>\n</g>\n<!-- 139902076319312&#45;&gt;139902076321744 -->\n<g class=\"edge\" id=\"edge9\">\n<title>139902076319312-&gt;139902076321744</title>\n<path d=\"M1207.0851,-1162.3799C1210.9643,-1153.907 1215.4299,-1144.1531 1219.6156,-1135.0107\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1222.8594,-1136.3334 1223.8399,-1125.784 1216.4947,-1133.4194 1222.8594,-1136.3334\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901716624 -->\n<g class=\"node\" id=\"node11\">\n<title>139901901716624</title>\n<polygon fill=\"none\" points=\"395,-1245.5 395,-1291.5 798,-1291.5 798,-1245.5 395,-1245.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500.5\" y=\"-1264.8\">tf.expand_dims_1: TFOpLambda</text>\n<polyline fill=\"none\" points=\"606,-1245.5 606,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"635\" y=\"-1276.3\">input:</text>\n<polyline fill=\"none\" points=\"606,-1268.5 664,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"635\" y=\"-1253.3\">output:</text>\n<polyline fill=\"none\" points=\"664,-1245.5 664,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"731\" y=\"-1276.3\">(None, None, 64)</text>\n<polyline fill=\"none\" points=\"664,-1268.5 798,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"731\" y=\"-1253.3\">(None, 1, None, 64)</text>\n</g>\n<!-- 139900642431760&#45;&gt;139901901716624 -->\n<g class=\"edge\" id=\"edge10\">\n<title>139900642431760-&gt;139901901716624</title>\n<path d=\"M595.7786,-1328.3799C595.8774,-1320.1745 595.9907,-1310.7679 596.0978,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"599.5986,-1301.8255 596.2195,-1291.784 592.5992,-1301.7411 599.5986,-1301.8255\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901723280 -->\n<g class=\"node\" id=\"node12\">\n<title>139901901723280</title>\n<polygon fill=\"none\" points=\"1027.5,-996.5 1027.5,-1042.5 1469.5,-1042.5 1469.5,-996.5 1027.5,-996.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1140.5\" y=\"-1015.8\">tf.__operators__.add: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1253.5,-996.5 1253.5,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1282.5\" y=\"-1027.3\">input:</text>\n<polyline fill=\"none\" points=\"1253.5,-1019.5 1311.5,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1282.5\" y=\"-1004.3\">output:</text>\n<polyline fill=\"none\" points=\"1311.5,-996.5 1311.5,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1390.5\" y=\"-1027.3\">(None, None, 1, 64)</text>\n<polyline fill=\"none\" points=\"1311.5,-1019.5 1469.5,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1390.5\" y=\"-1004.3\">(None, None, None, 64)</text>\n</g>\n<!-- 139902076321744&#45;&gt;139901901723280 -->\n<g class=\"edge\" id=\"edge11\">\n<title>139902076321744-&gt;139901901723280</title>\n<path d=\"M1238.3998,-1079.3799C1239.7838,-1071.1745 1241.3705,-1061.7679 1242.8699,-1052.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1246.3605,-1053.2269 1244.5726,-1042.784 1239.458,-1052.0625 1246.3605,-1053.2269\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642438672 -->\n<g class=\"node\" id=\"node14\">\n<title>139900642438672</title>\n<polygon fill=\"none\" points=\"1052.5,-913.5 1052.5,-959.5 1444.5,-959.5 1444.5,-913.5 1052.5,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1140.5\" y=\"-932.8\">tf.math.tanh: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1228.5,-913.5 1228.5,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1257.5\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"1228.5,-936.5 1286.5,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1257.5\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"1286.5,-913.5 1286.5,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1365.5\" y=\"-944.3\">(None, None, None, 64)</text>\n<polyline fill=\"none\" points=\"1286.5,-936.5 1444.5,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1365.5\" y=\"-921.3\">(None, None, None, 64)</text>\n</g>\n<!-- 139901901723280&#45;&gt;139900642438672 -->\n<g class=\"edge\" id=\"edge13\">\n<title>139901901723280-&gt;139900642438672</title>\n<path d=\"M1248.5,-996.3799C1248.5,-988.1745 1248.5,-978.7679 1248.5,-969.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1252.0001,-969.784 1248.5,-959.784 1245.0001,-969.784 1252.0001,-969.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642438864 -->\n<g class=\"node\" id=\"node15\">\n<title>139900642438864</title>\n<polygon fill=\"none\" points=\"1662.5,-1245.5 1662.5,-1291.5 1956.5,-1291.5 1956.5,-1245.5 1662.5,-1245.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1732.5\" y=\"-1264.8\">tf.cast: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1802.5,-1245.5 1802.5,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1831.5\" y=\"-1276.3\">input:</text>\n<polyline fill=\"none\" points=\"1802.5,-1268.5 1860.5,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1831.5\" y=\"-1253.3\">output:</text>\n<polyline fill=\"none\" points=\"1860.5,-1245.5 1860.5,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1908.5\" y=\"-1276.3\">(None, None)</text>\n<polyline fill=\"none\" points=\"1860.5,-1268.5 1956.5,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1908.5\" y=\"-1253.3\">(None, None)</text>\n</g>\n<!-- 139900642437968&#45;&gt;139900642438864 -->\n<g class=\"edge\" id=\"edge14\">\n<title>139900642437968-&gt;139900642438864</title>\n<path d=\"M1809.5,-1328.3799C1809.5,-1320.1745 1809.5,-1310.7679 1809.5,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1813.0001,-1301.784 1809.5,-1291.784 1806.0001,-1301.784 1813.0001,-1301.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901851728 -->\n<g class=\"node\" id=\"node16\">\n<title>139901901851728</title>\n<polygon fill=\"none\" points=\"1030.5,-830.5 1030.5,-876.5 1466.5,-876.5 1466.5,-830.5 1030.5,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1140.5\" y=\"-849.8\">tf.math.reduce_sum: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1250.5,-830.5 1250.5,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1279.5\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"1250.5,-853.5 1308.5,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1279.5\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"1308.5,-830.5 1308.5,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1387.5\" y=\"-861.3\">(None, None, None, 64)</text>\n<polyline fill=\"none\" points=\"1308.5,-853.5 1466.5,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1387.5\" y=\"-838.3\">(None, None, None)</text>\n</g>\n<!-- 139900642438672&#45;&gt;139901901851728 -->\n<g class=\"edge\" id=\"edge15\">\n<title>139900642438672-&gt;139901901851728</title>\n<path d=\"M1248.5,-913.3799C1248.5,-905.1745 1248.5,-895.7679 1248.5,-886.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1252.0001,-886.784 1248.5,-876.784 1245.0001,-886.784 1252.0001,-886.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901708368 -->\n<g class=\"node\" id=\"node17\">\n<title>139901901708368</title>\n<polygon fill=\"none\" points=\"1615,-1162.5 1615,-1208.5 2004,-1208.5 2004,-1162.5 1615,-1162.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1732.5\" y=\"-1181.8\">tf.math.reduce_sum_1: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1850,-1162.5 1850,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1879\" y=\"-1193.3\">input:</text>\n<polyline fill=\"none\" points=\"1850,-1185.5 1908,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1879\" y=\"-1170.3\">output:</text>\n<polyline fill=\"none\" points=\"1908,-1162.5 1908,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1956\" y=\"-1193.3\">(None, None)</text>\n<polyline fill=\"none\" points=\"1908,-1185.5 2004,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1956\" y=\"-1170.3\">(None,)</text>\n</g>\n<!-- 139900642438864&#45;&gt;139901901708368 -->\n<g class=\"edge\" id=\"edge16\">\n<title>139900642438864-&gt;139901901708368</title>\n<path d=\"M1809.5,-1245.3799C1809.5,-1237.1745 1809.5,-1227.7679 1809.5,-1218.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1813.0001,-1218.784 1809.5,-1208.784 1806.0001,-1218.784 1813.0001,-1218.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901136272 -->\n<g class=\"node\" id=\"node18\">\n<title>139901901136272</title>\n<polygon fill=\"none\" points=\"1034,-747.5 1034,-793.5 1463,-793.5 1463,-747.5 1034,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1152\" y=\"-766.8\">tf.compat.v1.transpose: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1270,-747.5 1270,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1299\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"1270,-770.5 1328,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1299\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"1328,-747.5 1328,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1395.5\" y=\"-778.3\">(None, None, None)</text>\n<polyline fill=\"none\" points=\"1328,-770.5 1463,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1395.5\" y=\"-755.3\">(None, None, None)</text>\n</g>\n<!-- 139901901851728&#45;&gt;139901901136272 -->\n<g class=\"edge\" id=\"edge17\">\n<title>139901901851728-&gt;139901901136272</title>\n<path d=\"M1248.5,-830.3799C1248.5,-822.1745 1248.5,-812.7679 1248.5,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1252.0001,-803.784 1248.5,-793.784 1245.0001,-803.784 1252.0001,-803.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901807440 -->\n<g class=\"node\" id=\"node19\">\n<title>139901901807440</title>\n<polygon fill=\"none\" points=\"1621,-1079.5 1621,-1125.5 1998,-1125.5 1998,-1079.5 1621,-1079.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1732.5\" y=\"-1098.8\">tf.sequence_mask_2: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1844,-1079.5 1844,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1873\" y=\"-1110.3\">input:</text>\n<polyline fill=\"none\" points=\"1844,-1102.5 1902,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1873\" y=\"-1087.3\">output:</text>\n<polyline fill=\"none\" points=\"1902,-1079.5 1902,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1950\" y=\"-1110.3\">None</text>\n<polyline fill=\"none\" points=\"1902,-1102.5 1998,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1950\" y=\"-1087.3\">(None, None)</text>\n</g>\n<!-- 139901901708368&#45;&gt;139901901807440 -->\n<g class=\"edge\" id=\"edge18\">\n<title>139901901708368-&gt;139901901807440</title>\n<path d=\"M1809.5,-1162.3799C1809.5,-1154.1745 1809.5,-1144.7679 1809.5,-1135.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1813.0001,-1135.784 1809.5,-1125.784 1806.0001,-1135.784 1813.0001,-1135.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901139344 -->\n<g class=\"node\" id=\"node21\">\n<title>139901901139344</title>\n<polygon fill=\"none\" points=\"1052,-664.5 1052,-710.5 1445,-710.5 1445,-664.5 1052,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1152\" y=\"-683.8\">tf.math.multiply: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1252,-664.5 1252,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1281\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"1252,-687.5 1310,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1281\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"1310,-664.5 1310,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1377.5\" y=\"-695.3\">(None, None, None)</text>\n<polyline fill=\"none\" points=\"1310,-687.5 1445,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1377.5\" y=\"-672.3\">(None, None, None)</text>\n</g>\n<!-- 139901901136272&#45;&gt;139901901139344 -->\n<g class=\"edge\" id=\"edge20\">\n<title>139901901136272-&gt;139901901139344</title>\n<path d=\"M1248.5,-747.3799C1248.5,-739.1745 1248.5,-729.7679 1248.5,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1252.0001,-720.784 1248.5,-710.784 1245.0001,-720.784 1252.0001,-720.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139902104268176 -->\n<g class=\"node\" id=\"node22\">\n<title>139902104268176</title>\n<polygon fill=\"none\" points=\"43,-1411.5 43,-1457.5 352,-1457.5 352,-1411.5 43,-1411.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5\" y=\"-1430.8\">tf.cast_1: TFOpLambda</text>\n<polyline fill=\"none\" points=\"198,-1411.5 198,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"227\" y=\"-1442.3\">input:</text>\n<polyline fill=\"none\" points=\"198,-1434.5 256,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"227\" y=\"-1419.3\">output:</text>\n<polyline fill=\"none\" points=\"256,-1411.5 256,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"304\" y=\"-1442.3\">(None, None)</text>\n<polyline fill=\"none\" points=\"256,-1434.5 352,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"304\" y=\"-1419.3\">(None, None)</text>\n</g>\n<!-- 139901901717264&#45;&gt;139902104268176 -->\n<g class=\"edge\" id=\"edge21\">\n<title>139901901717264-&gt;139902104268176</title>\n<path d=\"M198.9429,-1494.3799C198.7452,-1486.1745 198.5185,-1476.7679 198.3043,-1467.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"201.801,-1467.6968 198.0611,-1457.784 194.8031,-1467.8655 201.801,-1467.6968\" stroke=\"#000000\"/>\n</g>\n<!-- 139901901134992 -->\n<g class=\"node\" id=\"node23\">\n<title>139901901134992</title>\n<polygon fill=\"none\" points=\"1027.5,-581.5 1027.5,-627.5 1471.5,-627.5 1471.5,-581.5 1027.5,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1153\" y=\"-600.8\">tf.compat.v1.transpose_1: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1278.5,-581.5 1278.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1307.5\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"1278.5,-604.5 1336.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1307.5\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"1336.5,-581.5 1336.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1404\" y=\"-612.3\">(None, None, None)</text>\n<polyline fill=\"none\" points=\"1336.5,-604.5 1471.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1404\" y=\"-589.3\">(None, None, None)</text>\n</g>\n<!-- 139901901139344&#45;&gt;139901901134992 -->\n<g class=\"edge\" id=\"edge22\">\n<title>139901901139344-&gt;139901901134992</title>\n<path d=\"M1248.7786,-664.3799C1248.8774,-656.1745 1248.9907,-646.7679 1249.0978,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1252.5986,-637.8255 1249.2195,-627.784 1245.5992,-637.7411 1252.5986,-637.8255\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642488784 -->\n<g class=\"node\" id=\"node24\">\n<title>139900642488784</title>\n<polygon fill=\"none\" points=\"0,-1328.5 0,-1374.5 389,-1374.5 389,-1328.5 0,-1328.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"117.5\" y=\"-1347.8\">tf.math.reduce_sum_2: TFOpLambda</text>\n<polyline fill=\"none\" points=\"235,-1328.5 235,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-1359.3\">input:</text>\n<polyline fill=\"none\" points=\"235,-1351.5 293,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-1336.3\">output:</text>\n<polyline fill=\"none\" points=\"293,-1328.5 293,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"341\" y=\"-1359.3\">(None, None)</text>\n<polyline fill=\"none\" points=\"293,-1351.5 389,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"341\" y=\"-1336.3\">(None,)</text>\n</g>\n<!-- 139902104268176&#45;&gt;139900642488784 -->\n<g class=\"edge\" id=\"edge23\">\n<title>139902104268176-&gt;139900642488784</title>\n<path d=\"M196.6643,-1411.3799C196.3678,-1403.1745 196.0278,-1393.7679 195.7065,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"199.2006,-1384.651 195.3416,-1374.784 192.2052,-1384.9039 199.2006,-1384.651\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642489424 -->\n<g class=\"node\" id=\"node25\">\n<title>139900642489424</title>\n<polygon fill=\"none\" points=\"1027.5,-498.5 1027.5,-544.5 1471.5,-544.5 1471.5,-498.5 1027.5,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1153\" y=\"-517.8\">tf.compat.v1.transpose_2: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1278.5,-498.5 1278.5,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1307.5\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"1278.5,-521.5 1336.5,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1307.5\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"1336.5,-498.5 1336.5,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1404\" y=\"-529.3\">(None, None, None)</text>\n<polyline fill=\"none\" points=\"1336.5,-521.5 1471.5,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1404\" y=\"-506.3\">(None, None, None)</text>\n</g>\n<!-- 139901901134992&#45;&gt;139900642489424 -->\n<g class=\"edge\" id=\"edge24\">\n<title>139901901134992-&gt;139900642489424</title>\n<path d=\"M1249.5,-581.3799C1249.5,-573.1745 1249.5,-563.7679 1249.5,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1253.0001,-554.784 1249.5,-544.784 1246.0001,-554.784 1253.0001,-554.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642489808 -->\n<g class=\"node\" id=\"node26\">\n<title>139900642489808</title>\n<polygon fill=\"none\" points=\"0,-1245.5 0,-1291.5 377,-1291.5 377,-1245.5 0,-1245.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111.5\" y=\"-1264.8\">tf.sequence_mask_3: TFOpLambda</text>\n<polyline fill=\"none\" points=\"223,-1245.5 223,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252\" y=\"-1276.3\">input:</text>\n<polyline fill=\"none\" points=\"223,-1268.5 281,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252\" y=\"-1253.3\">output:</text>\n<polyline fill=\"none\" points=\"281,-1245.5 281,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-1276.3\">None</text>\n<polyline fill=\"none\" points=\"281,-1268.5 377,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-1253.3\">(None, None)</text>\n</g>\n<!-- 139900642488784&#45;&gt;139900642489808 -->\n<g class=\"edge\" id=\"edge25\">\n<title>139900642488784-&gt;139900642489808</title>\n<path d=\"M192.8287,-1328.3799C192.2355,-1320.1745 191.5555,-1310.7679 190.9129,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"194.3952,-1301.5056 190.1832,-1291.784 187.4134,-1302.0104 194.3952,-1301.5056\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642489872 -->\n<g class=\"node\" id=\"node27\">\n<title>139900642489872</title>\n<polygon fill=\"none\" points=\"1045.5,-415.5 1045.5,-461.5 1453.5,-461.5 1453.5,-415.5 1045.5,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1153\" y=\"-434.8\">tf.math.multiply_1: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1260.5,-415.5 1260.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1289.5\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"1260.5,-438.5 1318.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1289.5\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"1318.5,-415.5 1318.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1386\" y=\"-446.3\">(None, None, None)</text>\n<polyline fill=\"none\" points=\"1318.5,-438.5 1453.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1386\" y=\"-423.3\">(None, None, None)</text>\n</g>\n<!-- 139900642489424&#45;&gt;139900642489872 -->\n<g class=\"edge\" id=\"edge26\">\n<title>139900642489424-&gt;139900642489872</title>\n<path d=\"M1249.5,-498.3799C1249.5,-490.1745 1249.5,-480.7679 1249.5,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1253.0001,-471.784 1249.5,-461.784 1246.0001,-471.784 1253.0001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642490704 -->\n<g class=\"node\" id=\"node28\">\n<title>139900642490704</title>\n<polygon fill=\"none\" points=\"1027.5,-332.5 1027.5,-378.5 1471.5,-378.5 1471.5,-332.5 1027.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1153\" y=\"-351.8\">tf.compat.v1.transpose_3: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1278.5,-332.5 1278.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1307.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"1278.5,-355.5 1336.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1307.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"1336.5,-332.5 1336.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1404\" y=\"-363.3\">(None, None, None)</text>\n<polyline fill=\"none\" points=\"1336.5,-355.5 1471.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1404\" y=\"-340.3\">(None, None, None)</text>\n</g>\n<!-- 139900642489872&#45;&gt;139900642490704 -->\n<g class=\"edge\" id=\"edge27\">\n<title>139900642489872-&gt;139900642490704</title>\n<path d=\"M1249.5,-415.3799C1249.5,-407.1745 1249.5,-397.7679 1249.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1253.0001,-388.784 1249.5,-378.784 1246.0001,-388.784 1253.0001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642491152 -->\n<g class=\"node\" id=\"node29\">\n<title>139900642491152</title>\n<polygon fill=\"none\" points=\"1054.5,-249.5 1054.5,-295.5 1430.5,-295.5 1430.5,-249.5 1054.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1146\" y=\"-268.8\">tf.nn.softmax: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1237.5,-249.5 1237.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1266.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"1237.5,-272.5 1295.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1266.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"1295.5,-249.5 1295.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1363\" y=\"-280.3\">(None, None, None)</text>\n<polyline fill=\"none\" points=\"1295.5,-272.5 1430.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1363\" y=\"-257.3\">(None, None, None)</text>\n</g>\n<!-- 139900642490704&#45;&gt;139900642491152 -->\n<g class=\"edge\" id=\"edge28\">\n<title>139900642490704-&gt;139900642491152</title>\n<path d=\"M1247.5501,-332.3799C1246.8581,-324.1745 1246.0648,-314.7679 1245.3151,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1248.7918,-305.4545 1244.4637,-295.784 1241.8166,-306.0428 1248.7918,-305.4545\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642491600 -->\n<g class=\"node\" id=\"node30\">\n<title>139900642491600</title>\n<polygon fill=\"none\" points=\"1041,-166.5 1041,-212.5 1432,-212.5 1432,-166.5 1041,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1140\" y=\"-185.8\">tf.linalg.matmul: TFOpLambda</text>\n<polyline fill=\"none\" points=\"1239,-166.5 1239,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1268\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"1239,-189.5 1297,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1268\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"1297,-166.5 1297,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1364.5\" y=\"-197.3\">(None, None, None)</text>\n<polyline fill=\"none\" points=\"1297,-189.5 1432,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1364.5\" y=\"-174.3\">(None, None, 256)</text>\n</g>\n<!-- 139900642491152&#45;&gt;139900642491600 -->\n<g class=\"edge\" id=\"edge29\">\n<title>139900642491152-&gt;139900642491600</title>\n<path d=\"M1240.8287,-249.3799C1240.2355,-241.1745 1239.5555,-231.7679 1238.9129,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1242.3952,-222.5056 1238.1832,-212.784 1235.4134,-223.0104 1242.3952,-222.5056\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642491600&#45;&gt;139900642491984 -->\n<g class=\"edge\" id=\"edge30\">\n<title>139900642491600-&gt;139900642491984</title>\n<path d=\"M1201.6805,-166.3799C1187.3083,-156.8367 1170.4888,-145.6686 1155.2941,-135.5793\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1156.8331,-132.3999 1146.5663,-129.784 1152.961,-138.2314 1156.8331,-132.3999\" stroke=\"#000000\"/>\n</g>\n<!-- 139900642492048 -->\n<g class=\"node\" id=\"node32\">\n<title>139900642492048</title>\n<polygon fill=\"none\" points=\"962,-.5 962,-46.5 1261,-46.5 1261,-.5 962,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1015.5\" y=\"-19.8\">dense_2: Dense</text>\n<polyline fill=\"none\" points=\"1069,-.5 1069,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1098\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"1069,-23.5 1127,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1098\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"1127,-.5 1127,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1194\" y=\"-31.3\">(None, None, 512)</text>\n<polyline fill=\"none\" points=\"1127,-23.5 1261,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1194\" y=\"-8.3\">(None, None, 6000)</text>\n</g>\n<!-- 139900642491984&#45;&gt;139900642492048 -->\n<g class=\"edge\" id=\"edge32\">\n<title>139900642491984-&gt;139900642492048</title>\n<path d=\"M1111.5,-83.3799C1111.5,-75.1745 1111.5,-65.7679 1111.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1115.0001,-56.784 1111.5,-46.784 1108.0001,-56.784 1115.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtVmAVdFR65x",
        "outputId": "d590a346-f891-4172-b713-1c7ef5b1b77a"
      },
      "source": [
        "hitory=model.fit(dataset,epochs=20,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "472/472 [==============================] - 182s 334ms/step - loss: 1.2355 - accuracy: 0.2815\n",
            "Epoch 2/20\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.2310 - accuracy: 0.2829\n",
            "Epoch 3/20\n",
            "472/472 [==============================] - 158s 334ms/step - loss: 1.2266 - accuracy: 0.2842\n",
            "Epoch 4/20\n",
            "472/472 [==============================] - 158s 334ms/step - loss: 1.2221 - accuracy: 0.2857\n",
            "Epoch 5/20\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.2180 - accuracy: 0.2870\n",
            "Epoch 6/20\n",
            "472/472 [==============================] - 158s 336ms/step - loss: 1.2138 - accuracy: 0.2879\n",
            "Epoch 7/20\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.2096 - accuracy: 0.2892\n",
            "Epoch 8/20\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.2059 - accuracy: 0.2904\n",
            "Epoch 9/20\n",
            "472/472 [==============================] - 155s 329ms/step - loss: 1.2018 - accuracy: 0.2917\n",
            "Epoch 10/20\n",
            "472/472 [==============================] - 155s 328ms/step - loss: 1.1980 - accuracy: 0.2926\n",
            "Epoch 11/20\n",
            "472/472 [==============================] - 154s 327ms/step - loss: 1.1944 - accuracy: 0.2938\n",
            "Epoch 12/20\n",
            "472/472 [==============================] - 155s 329ms/step - loss: 1.1908 - accuracy: 0.2952\n",
            "472/472 [==============================] - 155s 329ms/step - loss: 1.1908 - accuracy: 0.2952\n",
            "Epoch 13/20\n",
            "Epoch 13/20\n",
            "472/472 [==============================] - 155s 329ms/step - loss: 1.1870 - accuracy: 0.2960\n",
            "472/472 [==============================] - 155s 329ms/step - loss: 1.1870 - accuracy: 0.2960\n",
            "Epoch 14/20\n",
            "Epoch 14/20\n",
            "472/472 [==============================] - 158s 334ms/step - loss: 1.1834 - accuracy: 0.2972\n",
            "472/472 [==============================] - 158s 334ms/step - loss: 1.1834 - accuracy: 0.2972\n",
            "Epoch 15/20\n",
            "Epoch 15/20\n",
            "472/472 [==============================] - 159s 336ms/step - loss: 1.1802 - accuracy: 0.2982\n",
            "472/472 [==============================] - 159s 336ms/step - loss: 1.1802 - accuracy: 0.2982\n",
            "Epoch 16/20\n",
            "Epoch 16/20\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1769 - accuracy: 0.2990\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1769 - accuracy: 0.2990\n",
            "Epoch 17/20\n",
            "Epoch 17/20\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1736 - accuracy: 0.3001\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1736 - accuracy: 0.3001\n",
            "Epoch 18/20\n",
            "Epoch 18/20\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1705 - accuracy: 0.3010\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1705 - accuracy: 0.3010\n",
            "Epoch 19/20\n",
            "Epoch 19/20\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.1672 - accuracy: 0.3020\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.1672 - accuracy: 0.3020\n",
            "Epoch 20/20\n",
            "Epoch 20/20\n",
            "472/472 [==============================] - 157s 332ms/step - loss: 1.1643 - accuracy: 0.3027\n",
            "472/472 [==============================] - 157s 332ms/step - loss: 1.1643 - accuracy: 0.3027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifiES05fnLzi"
      },
      "source": [
        "model.save('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/baseline1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_FxJ7UqscNP"
      },
      "source": [
        "from keras.models import load_model\n",
        "model=load_model('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/baseline1.h5')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW7YJiysjoMu"
      },
      "source": [
        "embed_weights=model.get_layer(index=0).get_weights()[0]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "NneM3LpjqP04",
        "outputId": "1954c32a-02de-492f-b0e3-71d2cb89e770"
      },
      "source": [
        "list_of_sentence[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the album was preceded by two of its singles run the world girls and best thing i never had which both attained moderate success.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP0YTSmLuQe1"
      },
      "source": [
        "test_input=texts_to_sequences(encode_data(list_of_sentence[100]))\n",
        "test_input.append(2)\n",
        "test_input=[1]+test_input\n",
        "test_1=test_input[:10]\n",
        "test_1=[test_1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZpuoE4YN4BU",
        "outputId": "df5bf753-0006-41f8-e580-0e505f0dfdb7"
      },
      "source": [
        "print([idx2word[i] for i in test_1[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', '▁the', '▁album', '▁was', '▁preced', 'ed', '▁by', '▁two', '▁of', '▁its', '▁own', '▁songs', '▁and', '▁the', '▁first', '▁of', '▁the', '▁first', '▁time', '.', '</s>', 'er', \"'\", 's', '▁first', '▁album', '▁was', '▁released', '▁in', '▁197']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxkI77fUq2M9",
        "outputId": "76641524-1387-40ab-b250-bba0e801bfdc"
      },
      "source": [
        "for i in range(20):\n",
        "  out=model.predict(test_1)\n",
        "  tok=np.argmax(out[:,-1,:],axis=-1)[0]\n",
        "  print(idx2word[tok])\n",
        "  test_1[0].append(tok)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "▁own\n",
            "▁songs\n",
            "▁and\n",
            "▁the\n",
            "▁first\n",
            "▁of\n",
            "▁the\n",
            "▁first\n",
            "▁time\n",
            ".\n",
            "</s>\n",
            "er\n",
            "'\n",
            "s\n",
            "▁first\n",
            "▁album\n",
            "▁was\n",
            "▁released\n",
            "▁in\n",
            "▁197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxNeEQYb_D1x",
        "outputId": "e0311c08-c9aa-4c8d-c42b-7c0d583d1e03"
      },
      "source": [
        "print([idx2word[i] for i in test_1[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', '▁the', '▁album', '▁was', '▁preced', 'ed', '▁by', '▁two', '▁of', '▁its', '▁own', '▁songs', '▁and', '▁the', '▁first', '▁of', '▁the', '▁first', '▁time', '.', '</s>', 'er', \"'\", 's', '▁first', '▁album', '▁was', '▁released', '▁in', '▁197']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "e-eoo3xD_O9Y",
        "outputId": "6624a7e1-edfa-419e-b54e-025188bf134c"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>ans_start</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>269</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>207</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>526</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>166</td>\n",
              "      <td>Houston, Texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>276</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Id    title  ... ans_start                 text\n",
              "0  56be85543aeaaa14008c9063  Beyoncé  ...       269    in the late 1990s\n",
              "1  56be85543aeaaa14008c9065  Beyoncé  ...       207  singing and dancing\n",
              "2  56be85543aeaaa14008c9066  Beyoncé  ...       526                 2003\n",
              "3  56bf6b0f3aeaaa14008c9601  Beyoncé  ...       166       Houston, Texas\n",
              "4  56bf6b0f3aeaaa14008c9602  Beyoncé  ...       276           late 1990s\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt3ofBWc_O6V"
      },
      "source": [
        "question_set=df.question.values\n",
        "answer_set=df.text.values"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-gFaHZ8_O3b"
      },
      "source": [
        "cleaned_question=[re.sub('[^A-Za-z0-9.\\' ]+','',i) for i in question_set]\n",
        "cleaned_question=[i.lower() for i in question_set]\n",
        "cleaned_answer=[re.sub('[^A-Za-z0-9.\\' ]+','',i) for i in answer_set]\n",
        "cleaned_answer=[i.lower() for i in answer_set]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIkT_rU6_O0e"
      },
      "source": [
        "question_tokens=[]\n",
        "answer_tokens=[]\n",
        "for i in cleaned_question:\n",
        "  encoded_data=encode_data(i)\n",
        "  tokens=texts_to_sequences(encoded_data)\n",
        "  tokens.append(2)\n",
        "  tokens=[1]+tokens\n",
        "  question_tokens.append(tokens)\n",
        "for i in cleaned_answer:\n",
        "  encoded_data=encode_data(i)\n",
        "  tokens=texts_to_sequences(encoded_data)\n",
        "  tokens.append(2)\n",
        "  tokens=[1]+tokens\n",
        "  answer_tokens.append(tokens)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXiyML7L_OxY"
      },
      "source": [
        "ragged_ques = tf.ragged.constant(question_tokens)\n",
        "ragged_ans= tf.ragged.constant(answer_tokens)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFQ1RscH_Oux"
      },
      "source": [
        "def process(ques, ans):\n",
        "    in_ans  = ans[:,:-1]\n",
        "    out_ans = ans[:,1:]\n",
        "    \n",
        "    in_ques  = ques.to_tensor(default_value=0, shape=[None, None])\n",
        "    in_ans  = in_ans.to_tensor(default_value=0, shape=[None, None])\n",
        "    out_ans = out_ans.to_tensor(default_value=0, shape=[None, None])\n",
        "    \n",
        "    return (in_ques, in_ans), out_ans\n",
        "dataset = tf.data.Dataset.from_tensor_slices((ragged_ques, ragged_ans))\n",
        "\n",
        "# Specify the batch size \n",
        "dataset = dataset.batch(256)\n",
        "\n",
        "# Use the .map() on the helper function defined above\n",
        "dataset = dataset.map(process, num_parallel_calls=4)\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tajvgiQ_Or0"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "inputs_ques = tf.keras.Input(shape=[None])\n",
        "inputs_ans = tf.keras.Input(shape=[None])\n",
        "\n",
        "emb_eng = tf.keras.layers.Embedding(input_dim=6000, \n",
        "                                    output_dim = 300,\n",
        "                                    weights=[embed_weights],\n",
        "                                    mask_zero=True,\n",
        "                                    trainable=False,\n",
        "                                    name='Encoder_embedding')\n",
        "emb_out=emb_eng(inputs_ques)\n",
        "encoder_rnn = tf.keras.layers.GRU(256, return_sequences=True, name='Encoder_RNN_f')\n",
        "encoder_states = encoder_rnn(emb_out)\n",
        "\n",
        "                                    \n",
        "last_encoder_state = encoder_states[:,-1,:]\n",
        "\n",
        "# Decoder\n",
        "emb_ans = tf.keras.layers.Embedding(input_dim=6000, \n",
        "                                    output_dim = 300,\n",
        "                                    weights=[embed_weights],\n",
        "                                    mask_zero=True,\n",
        "                                    trainable=False,\n",
        "                                    name='Decoder_embedding')(inputs_ans)\n",
        "\n",
        "\n",
        "decoder_rnn = tf.keras.layers.GRU(256, return_sequences=True,  name='Decoder_RNN')\n",
        "decoder_states = decoder_rnn(emb_ans, initial_state=last_encoder_state)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZDapERT_OpU"
      },
      "source": [
        "hidden_size=64\n",
        "\n",
        "dense_projection_e = tf.keras.layers.Dense(hidden_size, activation=None, use_bias=False)\n",
        "dense_projection_d = tf.keras.layers.Dense(hidden_size, activation=None, use_bias=False)\n",
        "\n",
        "r_encoder = dense_projection_e(encoder_states)\n",
        "r_decoder = dense_projection_d(decoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHzKlIx_Ol-"
      },
      "source": [
        "# Step 1:\n",
        "# Expand the dimensions for the query and the value at the appropriate locations \n",
        "query = tf.expand_dims(r_decoder, axis=2)\n",
        "value = tf.expand_dims(r_encoder, axis=1)\n",
        "\n",
        "\n",
        "# Step 2:\n",
        "# #Compute the score as in Bahdanau's paper  \n",
        "score = tf.reduce_sum(tf.tanh(query + value, name='Score'), axis=-1)\n",
        "\n",
        "\n",
        "# Additional step:\n",
        "# It is crucial to manually mask the 'padded' hidden states otherwise there will be a mismatch between the encoder-decoder model defined above\n",
        "# And the attention weights computed below \n",
        "N_query = tf.reduce_sum(tf.cast(inputs_ans>0, tf.float32), axis=1)\n",
        "query_mask = tf.sequence_mask(N_query)\n",
        "\n",
        "N_value = tf.reduce_sum(tf.cast(inputs_ques>0, tf.float32), axis=1)\n",
        "value_mask = tf.sequence_mask(N_value)\n",
        "# # Mask the score\n",
        "m1 = tf.sequence_mask(N_query, dtype=tf.float32)\n",
        "m2 = tf.sequence_mask(N_value, dtype=tf.float32)\n",
        "\n",
        "score = tf.transpose(tf.multiply(tf.transpose(score,[2,0,1]), m1),[1,2,0])\n",
        "score = tf.transpose(tf.multiply(tf.transpose(score,[1,0,2]), m2), [1,0,2])\n",
        "\n",
        "# Step 3: Use the scores to get a distribution of probabilities\n",
        "# Hint: Use softmax \n",
        "weights = tf.nn.softmax(score, axis=2, name='Weights')\n",
        "\n",
        "# Step 4: Get the context vector by multiplying the encoder states with the weights \n",
        "# # Compute the context\n",
        "context_vector = tf.matmul(weights, encoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVvzmca8Lwp1"
      },
      "source": [
        "enhanced_encoding = tf.keras.layers.Concatenate(axis=2)([context_vector, decoder_states])\n",
        "\n",
        "# Pass the enhanced encoding to a dense layer with softmax activation \n",
        "output_ans = tf.keras.layers.Dense(6000,activation='softmax')(enhanced_encoding)\n",
        "\n",
        "\n",
        "# Set up the model with appropriate inputs and the output defined above \n",
        "model2 = tf.keras.Model(inputs=(inputs_ques, inputs_ans), outputs=output_ans)\n",
        "\n",
        "# Choose an appropriate learning rate and optimizer\n",
        "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPaq4djdLwm4",
        "outputId": "f9d1067d-2521-4ef8-8c9c-7a7a3c0067a1"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_embedding (Embedding)   (None, None, 300)    1800000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_RNN_f (GRU)             (None, None, 256)    428544      Encoder_embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_embedding (Embedding)   (None, None, 300)    1800000     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 256)          0           Encoder_RNN_f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_RNN (GRU)               (None, None, 256)    428544      Decoder_embedding[0][0]          \n",
            "                                                                 tf.__operators__.getitem[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 64)     16384       Decoder_RNN[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 64)     16384       Encoder_RNN_f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims (TFOpLambda)     (None, None, 1, 64)  0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_1 (TFOpLambda)   (None, 1, None, 64)  0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (None, None, None, 6 0           tf.expand_dims[0][0]             \n",
            "                                                                 tf.expand_dims_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.greater (TFOpLambda)    (None, None)         0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.tanh (TFOpLambda)       (None, None, None, 6 0           tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.cast (TFOpLambda)            (None, None)         0           tf.math.greater[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum (TFOpLambda) (None, None, None)   0           tf.math.tanh[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum_1 (TFOpLambd (None,)              0           tf.cast[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose (TFOpLam (None, None, None)   0           tf.math.reduce_sum[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.sequence_mask_2 (TFOpLambda) (None, None)         0           tf.math.reduce_sum_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.greater_1 (TFOpLambda)  (None, None)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply (TFOpLambda)   (None, None, None)   0           tf.compat.v1.transpose[0][0]     \n",
            "                                                                 tf.sequence_mask_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.cast_1 (TFOpLambda)          (None, None)         0           tf.math.greater_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_1 (TFOpL (None, None, None)   0           tf.math.multiply[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum_2 (TFOpLambd (None,)              0           tf.cast_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_2 (TFOpL (None, None, None)   0           tf.compat.v1.transpose_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.sequence_mask_3 (TFOpLambda) (None, None)         0           tf.math.reduce_sum_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply_1 (TFOpLambda) (None, None, None)   0           tf.compat.v1.transpose_2[0][0]   \n",
            "                                                                 tf.sequence_mask_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_3 (TFOpL (None, None, None)   0           tf.math.multiply_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.softmax (TFOpLambda)      (None, None, None)   0           tf.compat.v1.transpose_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul (TFOpLambda)   (None, None, 256)    0           tf.nn.softmax[0][0]              \n",
            "                                                                 Encoder_RNN_f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, None, 512)    0           tf.linalg.matmul[0][0]           \n",
            "                                                                 Decoder_RNN[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 6000)   3078000     concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 7,567,856\n",
            "Trainable params: 3,967,856\n",
            "Non-trainable params: 3,600,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXXHiX43MQdH",
        "outputId": "c3569941-e818-4287-861f-fc78d46730c9"
      },
      "source": [
        "history2=model2.fit(dataset,epochs=20,verbose=1)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "340/340 [==============================] - 22s 65ms/step - loss: 0.4404\n",
            "Epoch 2/20\n",
            "340/340 [==============================] - 22s 65ms/step - loss: 0.4347\n",
            "Epoch 3/20\n",
            "340/340 [==============================] - 23s 66ms/step - loss: 0.4291\n",
            "Epoch 4/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4246\n",
            "Epoch 5/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4216\n",
            "Epoch 6/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4171\n",
            "Epoch 7/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4128\n",
            "Epoch 8/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4093\n",
            "Epoch 9/20\n",
            "340/340 [==============================] - 23s 68ms/step - loss: 0.4069\n",
            "Epoch 10/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4035\n",
            "Epoch 11/20\n",
            "340/340 [==============================] - 23s 68ms/step - loss: 0.4010\n",
            "Epoch 12/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.4024\n",
            "Epoch 13/20\n",
            "340/340 [==============================] - 23s 68ms/step - loss: 0.3966\n",
            "Epoch 14/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.3928\n",
            "Epoch 15/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.3904\n",
            "Epoch 16/20\n",
            "340/340 [==============================] - 23s 68ms/step - loss: 0.3887\n",
            "Epoch 17/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.3846\n",
            "Epoch 18/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.3813\n",
            "Epoch 19/20\n",
            "340/340 [==============================] - 23s 68ms/step - loss: 0.3775\n",
            "Epoch 20/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.3739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7qNga6Ns60b"
      },
      "source": [
        "test_ques=df.question.iloc[15]\n",
        "test_ques=texts_to_sequences(encode_data(test_ques))\n",
        "test_ques.append(2)\n",
        "test_ques=[1]+test_ques\n",
        "test_ques=[test_ques]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CfBjbqPPSGQ",
        "outputId": "eabdef7f-e614-4354-87cc-e12ade46dd3f"
      },
      "source": [
        "print([idx2word[i] for i in test_ques[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', '▁', '<unk>', 'f', 'ter', '▁her', '▁second', '▁sol', 'o', '▁album', '<unk>', '▁what', '▁other', '▁entertainment', '▁vent', 'ure', '▁did', '▁', '<unk>', 'ey', 'once', '▁expl', 'ore', '<unk>', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A45q-w8RsDt4"
      },
      "source": [
        "model2.save('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/encode.h5')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1UYBrW1Qls2"
      },
      "source": [
        "from keras.models import load_model\n",
        "model2=load_model('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/encode.h5')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVo5-35Ki1di"
      },
      "source": [
        "def evaluate(test_ques):\n",
        "  print(test_ques)\n",
        "  sentence = texts_to_sequences(encode_data(test_ques))\n",
        "  START_TOKEN=[word2idx['<s>']]\n",
        "  END_TOKEN=[word2idx['</s>']]\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + sentence + END_TOKEN, axis=0)\n",
        "  output = tf.expand_dims(START_TOKEN, 0)\n",
        "  for i in range(20):\n",
        "    predictions = model2(inputs=[sentence, output], training=False)\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # concatenated the predicted_id to the output which is given to the decoder as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)\n",
        "\n",
        "def predict(sentence):\n",
        "  prediction = evaluate(sentence)\n",
        "  predicted_sentence = [idx2word[i] for i in prediction.numpy()]\n",
        "  return predicted_sentence"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKJfR0smjIJX",
        "outputId": "ffacd709-ada3-458c-a566-879052ee1c37"
      },
      "source": [
        "for i in range(0,100,5):\n",
        "  print(predict(df.question.iloc[i]))\n",
        "  print(df.text.iloc[i])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When did Beyonce start becoming popular?\n",
            "['<s>', '▁n', '<unk>', 's', '▁n', 'ied', 'er', 'ts']\n",
            "in the late 1990s\n",
            "In what R&B group was she the lead singer?\n",
            "['<s>', '▁african', '▁american', '▁tribes']\n",
            "Destiny's Child\n",
            "What was the first album Beyoncé released as a solo artist?\n",
            "['<s>', '▁the', '▁tr', 'ips']\n",
            "Dangerously in Love\n",
            "After her second solo album, what other entertainment venture did Beyonce explore?\n",
            "['<s>', '▁inf', 'in', 'ite']\n",
            "acting\n",
            "Which album was darker in tone from her previous work?\n",
            "['<s>', '▁aristot', 'ians']\n",
            "Beyoncé\n",
            "Who is Beyoncé married to?\n",
            "['<s>', '▁engine', 'er']\n",
            "Jay Z\n",
            "In which decade did the Recording Industry Association of America recognize Beyonce as the The Top Certified Artist?\n",
            "['<s>', '▁early', '▁1990', 's']\n",
            "2000s\n",
            "How many records did Beyoncé sell as part of Destiny's Child?\n",
            "['<s>', '▁the', '▁su', 'e', 'z', '▁canal']\n",
            "60 million\n",
            "Where did Beyonce get her name from?\n",
            "['<s>', '▁to', '▁use', '▁a', '▁th', 'ing', '<unk>', '▁sal', 't', '<unk>', 'day', '▁his', '▁case']\n",
            "her mother's maiden name\n",
            "What younger sister of Beyonce also appeared in Destiny's Child?\n",
            "['<s>', '▁most', '▁widespread']\n",
            "Solange\n",
            "Beyoncé is a descendant of which Acadian leader?\n",
            "['<s>', '▁sh', 'ir', 'rel']\n",
            "Joseph Broussard.\n",
            "Which of her teachers discovered Beyonce's musical talent?\n",
            "['<s>', '▁cap', 'ric', 'ain']\n",
            "dance instructor Darlette Johnson\n",
            "What was the name of Beyoncé's first dance instructor?\n",
            "['<s>', '▁john', '▁i', \"'\", 's', '▁still', '▁be', '▁pred', 'icted']\n",
            "Darlette Johnson\n",
            "Who was the first record label to give the girls a record deal?\n",
            "['<s>', '▁sym', 'ph', 'ony']\n",
            "Elektra Records\n",
            "At what age did Beyonce meet LaTavia Robertson?\n",
            "['<s>', '▁5', '<unk>']\n",
            "age eight\n",
            "Who signed the girl group on October 5, 1995?\n",
            "['<s>', '▁super', 'com', 'mon', '▁protection', '▁service']\n",
            "Dwayne Wiggins's Grass Roots Entertainment\n",
            "The name Destiny's Child was based on a quote in which book of the Bible?\n",
            "['<s>', '▁w', 'es']\n",
            "Book of Isaiah\n",
            "Destiny's Child song, Killing Time, was included in which film's soundtrack?\n",
            "['<s>', '▁rob', 'et', '▁bar', 'rel', 's']\n",
            "Men in Black.\n",
            "What event occured after she was publicly criticized?\n",
            "['<s>', '▁an', '▁threat', 's']\n",
            "boyfriend left her\n",
            "Who replaced Luckett and Roberson in Destiny's Child?\n",
            "['<s>', '▁mar', 's']\n",
            "Farrah Franklin and Michelle Williams.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaa2uWh_jIFn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}