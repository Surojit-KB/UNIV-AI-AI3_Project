{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surojit-KB/UNIV-AI-AI3_Project/blob/main/Baseline_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViM5hHKyG64P"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ2WxVPn06oG"
      },
      "source": [
        "#IMPORTING THE NECESSARY LIBRARIES\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Embedding, Dense, TimeDistributed,LSTM, GRU,Dropout, Bidirectional, Conv1D, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmc7WcDt_gx4",
        "outputId": "a861f190-f469-45c7-8e6d-05b624feb477"
      },
      "source": [
        "#INSTALLING THE SENTENCEPIECE LIBRARY FOR SENTENCEPIECE TOKENIZER\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 33.3MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 35.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 33.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 35.9MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 26.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 27.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 28.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 25.4MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 25.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 25.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 25.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 25.4MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 25.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 25.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 25.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 25.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kji07lM1QdY",
        "outputId": "80266757-1fee-40c5-ba8f-b380082304b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X94_u0H2PYFa"
      },
      "source": [
        "## 2. Getting the data and preprocessing\n",
        "The SQUAD dataset is in the form of a JSON file, hence we need to load it manually into a dataframe. This part of the notebook focuses on loading the dataframe, preprocessing the data and getting the context text ready for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-sJeZUHIzc_"
      },
      "source": [
        "#THIS IS THE CODE TO PREPROCESS THE SQUAD DATASET AND CREATE A DATAFRAME OUT OF IT\n",
        "def get_dataframe(file):\n",
        "  f = open(file, 'r')\n",
        "  #loading json file \n",
        "  data = json.loads(f.read())\n",
        "  #creating empty lists to store df values \n",
        "  iid = []\n",
        "  tit = []\n",
        "  con = []\n",
        "  que = []\n",
        "  ans = []\n",
        "  txt = []\n",
        "  #root tags contains 'title' tag and 'paragraphs' list \n",
        "  for i in range(len(data['data'])):\n",
        "    title = data['data'][i]['title']\n",
        "    #'paragraphs' list contains 'context' tag and 'qas' list \n",
        "    for p in range(len(data['data'][i]['paragraphs'])):\n",
        "      context = data['data'][i]['paragraphs'][p]['context']\n",
        "      for q in range(len(data['data'][i]['paragraphs'][p]['qas'])):\n",
        "        # 'qas'list contains 'question', 'Id' tag and 'answers' list \n",
        "        question = data['data'][i]['paragraphs'][p]['qas'][q]['question']\n",
        "        id = data['data'][i]['paragraphs'][p]['qas'][q]['id']\n",
        "        #'answers' list contains 'answer_start' and 'text' tag \n",
        "        for a in range(len(data['data'][i]['paragraphs'][p]['qas'][q]['answers'])):\n",
        "          ans_start = data['data'][i]['paragraphs'][p]['qas'][q]['answers'][a]['answer_start']\n",
        "          text = data['data'][i]['paragraphs'][p]['qas'][q]['answers'][a]['text']\n",
        "          \n",
        "          #appending values to list \n",
        "          iid.append(id)\n",
        "          tit.append(title)\n",
        "          con.append(context)\n",
        "          que.append(question)\n",
        "          ans.append(ans_start)\n",
        "          txt.append(text)\n",
        "  #creating dataframe from lists \n",
        "  new_df = pd.DataFrame(columns=['Id', 'title', 'context', 'question', 'ans_start', 'text'])\n",
        "  new_df.Id = iid\n",
        "  new_df.title=tit\n",
        "  new_df.context = con\n",
        "  new_df.question = que\n",
        "  new_df.ans_start = ans \n",
        "  new_df.text = txt \n",
        "  #removing duplicate columns \n",
        "  final_df = new_df.drop_duplicates(keep='first')\n",
        "\n",
        "  return final_df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVjCaABR1QbA"
      },
      "source": [
        "#GETTING THE DATAFRAME READY\n",
        "df=get_dataframe('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/train-v2.0.json')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "lILTaiexBCHh",
        "outputId": "7272533f-ebf8-431d-af35-2fdf5855abdf"
      },
      "source": [
        "#DISPLAY THE DATAFRAME\n",
        "df.head(10)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>ans_start</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>269</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>207</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>526</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>166</td>\n",
              "      <td>Houston, Texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>276</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9603</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In what R&amp;B group was she the lead singer?</td>\n",
              "      <td>320</td>\n",
              "      <td>Destiny's Child</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9604</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What album made her a worldwide known artist?</td>\n",
              "      <td>505</td>\n",
              "      <td>Dangerously in Love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9605</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Who managed the Destiny's Child group?</td>\n",
              "      <td>360</td>\n",
              "      <td>Mathew Knowles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>56d43c5f2ccc5a1400d830a9</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyoncé rise to fame?</td>\n",
              "      <td>276</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>56d43c5f2ccc5a1400d830aa</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What role did Beyoncé have in Destiny's Child?</td>\n",
              "      <td>290</td>\n",
              "      <td>lead singer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Id    title  ... ans_start                 text\n",
              "0  56be85543aeaaa14008c9063  Beyoncé  ...       269    in the late 1990s\n",
              "1  56be85543aeaaa14008c9065  Beyoncé  ...       207  singing and dancing\n",
              "2  56be85543aeaaa14008c9066  Beyoncé  ...       526                 2003\n",
              "3  56bf6b0f3aeaaa14008c9601  Beyoncé  ...       166       Houston, Texas\n",
              "4  56bf6b0f3aeaaa14008c9602  Beyoncé  ...       276           late 1990s\n",
              "5  56bf6b0f3aeaaa14008c9603  Beyoncé  ...       320      Destiny's Child\n",
              "6  56bf6b0f3aeaaa14008c9604  Beyoncé  ...       505  Dangerously in Love\n",
              "7  56bf6b0f3aeaaa14008c9605  Beyoncé  ...       360       Mathew Knowles\n",
              "8  56d43c5f2ccc5a1400d830a9  Beyoncé  ...       276           late 1990s\n",
              "9  56d43c5f2ccc5a1400d830aa  Beyoncé  ...       290          lead singer\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1cij-7iMc2t",
        "outputId": "7663efb2-4ac7-4702-ed5f-39ef7d569122"
      },
      "source": [
        "#INFO ABOUT THE DATASET\n",
        "print('The number of different titles in the SQUAD dataset are:',df.title.nunique())\n",
        "print('The number of different context paragraphs in the SQUAD dataset are:',df.context.nunique())\n",
        "print('The number of unique question-answer pairs in the SQUAD dataset are:',df.question.nunique())"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of different titles in the SQUAD dataset are: 440\n",
            "The number of different context paragraphs in the SQUAD dataset are: 18877\n",
            "The number of unique question-answer pairs in the SQUAD dataset are: 86769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg2GLiuNQ6-o"
      },
      "source": [
        "#NEXT WE WILL CREATE A TEXT STRING OUT OF ALL THE UNIQUE CONTEXT PARAGRAPHS WE HAVE\n",
        "#THIS TEXT WILL BE USED TO TRAIN A SIMPLE BASELINE MODEL\n",
        "text=''.join(df.context.unique())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wpzdhSpJCD6"
      },
      "source": [
        "#SINCE THE DATA CONTAINS A LOT OF PHONETIC CHARACTERS, WE WILL CONVERT THEM TO NORMAL CHARACTERS FOR SAKE OF BETTER ENCODING\n",
        "#THIS WILL HELP US IN BETTER TOKENIZATION\n",
        "import unicodedata\n",
        "normalized_text=str(unicodedata.normalize('NFD', text).encode('ascii', 'ignore'))\n",
        "#FURTHER CLEANING TO REMOVE THE SPECIAL CHARACTERS AND LOWERCASE THE TEXT\n",
        "cleaned_text=re.sub('[^A-Za-z0-9.\\' ]+','',normalized_text[2:])\n",
        "cleaned_text=cleaned_text.lower()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHxhlie2Vl68",
        "outputId": "7c3bbc80-1a7a-48da-82dd-2971abcd22b1"
      },
      "source": [
        "#TO CONVERT THE TEXT INTO SENTENCES, WE ARE USING A TOKENIZER FROM THE NLTK LIBRARY.\n",
        "#THE TEXT THAT WE HAVE HAS QUITE MANY PERIODS WHICH DO NOT ALWAYS DENOTE THE END OF A SENTENCE\n",
        "#NLTK TOKENIZER IS BEST SUITED FOR THIS PURPOSE\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "list_of_sentence=tokenizer.tokenize(cleaned_text)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HfTBSgMNZBL",
        "outputId": "55864c69-62e9-4c50-fa58-9d5870600822"
      },
      "source": [
        "#EXAMPLE OUTPUT OF THE TOKENIZER FOR CREATING A LIST OF SENTENCE\n",
        "list_of_sentence[:10]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['beyonce giselle knowlescarter bijnse beeyonsay born september 4 1981 is an american singer songwriter record producer and actress.',\n",
              " \"born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of rb girlgroup destiny's child.\",\n",
              " \"managed by her father mathew knowles the group became one of the world's bestselling girl groups of all time.\",\n",
              " \"their hiatus saw the release of beyonce's debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 numberone singles crazy in love and baby boy.following the disbandment of destiny's child in june 2005 she released her second solo album b'day 2006 which contained hits deja vu irreplaceable and beautiful liar.\",\n",
              " 'beyonce also ventured into acting with a golden globenominated performance in dreamgirls 2006 and starring roles in the pink panther 2006 and obsessed 2009. her marriage to rapper jay z and portrayal of etta james in cadillac records 2008 influenced her third album i am... sasha fierce 2008 which saw the birth of her alterego sasha fierce and earned a recordsetting six grammy awards in 2010 including song of the year for single ladies put a ring on it.',\n",
              " 'beyonce took a hiatus from music in 2010 and took over management of her career her fourth album 4 2011 was subsequently mellower in tone exploring 1970s funk 1980s pop and 1990s soul.',\n",
              " 'her critically acclaimed fifth studio album beyonce 2013 was distinguished from previous releases by its experimental production and exploration of darker themes.a selfdescribed modernday feminist beyonce creates songs that are often characterized by themes of love relationships and monogamy as well as female sexuality and empowerment.',\n",
              " 'on stage her dynamic highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music.',\n",
              " \"throughout a career spanning 19 years she has sold over 118 million records as a solo artist and a further 60 million with destiny's child making her one of the bestselling music artists of all time.\",\n",
              " \"she has won 20 grammy awards and is the most nominated woman in the award's history.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQQlBebqP-8l"
      },
      "source": [
        "## 3. TOKENIZATION\n",
        "Next we will tokenize the context data using the SentencePiece Tokenizer. \n",
        "\n",
        "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing. More info can be found [here](https://github.com/google/sentencepiece).\n",
        "\n",
        "We will use a tokenizer with byte-pair-encoding.\n",
        "The general idea of byte-pair-encoding is to do the following ([source](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)):\n",
        "\n",
        "1. Go through your corpus and find all the “bytes” i.e. the irreducible characters from which all others can be built. This is our base. It ensures we can almost always reconstruct any unseen input.\n",
        "2. Run a sliding window over the entire corpus (the actual code is slightly different) and find the most frequent bigram. Bigrams are formed from consecutive subwords in the current list of seen subwords. So, “hello” would have the counts {“he”: 1, “el”:1, “ll”:1, “lo”: 1}.\n",
        "3. Choose the most frequent bigram, add it to the list of subwords, then merge all instances of this bigram in the corpus.\n",
        "4. Repeat until you reach your desired vocabulary size.\n",
        "\n",
        "The vocabulary size is fixed at 6000 for the complete baseline modelling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoFkXlsY__v3"
      },
      "source": [
        "#WE WILL SAVE ALL THE SENTENCES IN A TEXT FILE, WHICH WILL BE PROVIDED THE TOKENIZER TO WORK ON\n",
        "rect_path='./context.txt'\n",
        "with open(rect_path, 'w', errors = 'ignore') as file:\n",
        "    for line in list_of_sentence:\n",
        "      file.write(line+'\\n')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDs0AJn--cX"
      },
      "source": [
        "#DEFINING THE TOKENIZER\n",
        "def sentencepiece_():\n",
        "  templates= '--input={} \\\n",
        "  --pad_id={} \\\n",
        "  --bos_id={} \\\n",
        "  --eos_id={} \\\n",
        "  --unk_id={} \\\n",
        "  --model_prefix={} \\\n",
        "  --vocab_size={} \\\n",
        "  --character_coverage={} \\\n",
        "  --model_type={}'\n",
        "\n",
        "  train_input_file = rect_path\n",
        "  prefix = './en_spm' \n",
        "  pad_id=0  #PAD ID\n",
        "  vocab_size = 6000 \n",
        "  bos_id=1  #BEGIN OF SENTENCE ID\n",
        "  eos_id=2  #END OF SENTENCE ID\n",
        "  unk_id=3  #UNKNOWN ID\n",
        "  character_coverage = 1.0 \n",
        "  model_type ='bpe' \n",
        "\n",
        "\n",
        "  cmd = templates.format(train_input_file,\n",
        "                pad_id,\n",
        "                bos_id,\n",
        "                eos_id,\n",
        "                unk_id,\n",
        "                prefix,\n",
        "                vocab_size,\n",
        "                character_coverage,\n",
        "                model_type)\n",
        " \n",
        "  spm.SentencePieceTrainer.train(cmd) #TRAINING THE TOKENIZER\n",
        "\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  sp.load('./en_spm.model')\n",
        "  return sp"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJsV7uAnApIt"
      },
      "source": [
        "sp=sentencepiece_() #TRAINED TOKENIZER"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXwXLZdo-L6q"
      },
      "source": [
        "#NEXT WE GET THE VOCABULARY OUT OF THE TOKENIZER MODEL\n",
        "#WE WILL CREATE A WORD TO INDEX AND INDEX TO WORD DICTIONARY FOR FURTHER USE\n",
        "def dictionary():\n",
        "  with open('./en_spm.vocab', encoding='utf-8') as f:\n",
        "      Vo = [doc.strip().split(\"\\t\") for doc in f]\n",
        "\n",
        "  # w[0]: token name    \n",
        "  # w[1]: token score \n",
        "  word2idx_en = {w[0]: i for i, w in enumerate(Vo)}\n",
        "  idx2word_en = {i:w[0] for i, w in enumerate(Vo)}\n",
        "\n",
        "  return word2idx_en, idx2word_en\n",
        "word2idx,idx2word=dictionary()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5jGS5m-CcE7"
      },
      "source": [
        "#NOW, WE CREATE FUNCTIONS FOR ENCODING THE INPUT TEXT AND CONVERTING THEM TO SEQUENCES\n",
        "#ENCODING WILL SEPARATE THE WORDS IN THE TEXT INTO SUB-WORDS AS PER THE TOKENIZER VOCAB\n",
        "#THEN WE WILL ENCODE THESE SUB-WORDS INTO TOKENS BASED ON VOCAB\n",
        "def encode_data(input):\n",
        "            pieces = sp.encode_as_pieces(input)\n",
        "            return pieces\n",
        "def texts_to_sequences(texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            text_list = text.split()\n",
        "            sequence = [int(word2idx.get(token, word2idx[\"<unk>\"])) for token in text_list]\n",
        "            sequences += sequence\n",
        "        return sequences"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFf5UNv7C1Vl"
      },
      "source": [
        "#HERE WE CREATE THE TOKEN LIST FOR ALL THE SENTENCES WE HAVE\n",
        "token_list=[]\n",
        "for i in list_of_sentence:\n",
        "  encoded_data=encode_data(i)\n",
        "  tokens=texts_to_sequences(encoded_data)\n",
        "  tokens.append(2) #ADDING THE END TOKEN TO THE TOKEN-LIST\n",
        "  tokens=[1]+tokens #ADDING THE START TOKEN TO THE TOKEN-LIST\n",
        "  token_list.append(tokens)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_aJE8vmug-_",
        "outputId": "053a600e-c17a-427a-c141-78e3c1665d66"
      },
      "source": [
        "print('The MIN sequence length of the input data is:',np.min([len(i) for i in token_list]))\n",
        "print('The MAX sequence length of the input data is:',np.max([len(i) for i in token_list]))\n",
        "print('The mean sequence length of the input data is:',np.mean([len(i) for i in token_list]))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The MIN sequence length of the input data is: 3\n",
            "The MAX sequence length of the input data is: 738\n",
            "The mean sequence length of the input data is: 48.01034322009207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T32Qa4mLMjyY"
      },
      "source": [
        "#TO MAKE SURE WE HAVE SUFFICIENT DATA FOR TRAINING WITHOUT HAVING TOO MUCH PADDED SEQUENCES,\n",
        "#THE MINIMUM SEQUENCE LENTGH WAS CONSIDERED TO BE 20 AND THE MAX SEQUENCE LENGTH WAS CONSIDERED TO BE 250\n",
        "cleaned_list=[]\n",
        "for i in token_list:\n",
        "  if len(i)>20:\n",
        "    if len(i)>250:\n",
        "      cleaned_list.append(i[:250]+[2])\n",
        "    else:\n",
        "      cleaned_list.append(i)\n",
        "maxlen=len(max(cleaned_list,key=len))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcT9VhxjvAXo",
        "outputId": "c027d350-c6b8-49ff-9f6e-e6082b51000d"
      },
      "source": [
        "print('The maximum sequence length of the input data is:',maxlen)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The maximum sequence length of the input data is: 251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZwdLtslvOi7",
        "outputId": "48b7ba72-471f-4b6f-8c52-d7720cdd40cb"
      },
      "source": [
        "#LETS SEE WHAT AN INPUT SENTENCE LOOKS LIKE IN TERMS OF THE ENCODING\n",
        "print([idx2word[i] for i in token_list[8]])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', '▁throughout', '▁a', '▁career', '▁sp', 'an', 'ning', '▁19', '▁years', '▁she', '▁has', '▁sold', '▁over', '▁11', '8', '▁million', '▁records', '▁as', '▁a', '▁sol', 'o', '▁artist', '▁and', '▁a', '▁further', '▁60', '▁million', '▁with', '▁dest', 'iny', \"'\", 's', '▁child', '▁making', '▁her', '▁one', '▁of', '▁the', '▁best', 'se', 'lling', '▁music', '▁artists', '▁of', '▁all', '▁time', '.', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do_24FwWVwDH"
      },
      "source": [
        "## 4. MODELLING THE BASELINE LANGUAGE MODEL\n",
        "Now we will creata a baseline language model which will be trained on the context data and provide us with the embeddings for the vocabulary. It is a very basic Sequential model consisting of:\n",
        "1. An Embedding Layer of dimension 300\n",
        "2. An LSTM layer of dimension 512\n",
        "3. An Dropout Layer with dropout rate of 0.2\n",
        "4. A second LSTM layer of dimension 512\n",
        "5. A second Dropout Layer with dropout rate of 0.2\n",
        "6. A Dense layer of vocab size with softmax activation\n",
        "\n",
        "The batch size of the dataset is 128, the model was trained for around 40 epochs in 4 rounds (since it took a long time to train the epochs, the model was trained for 10 epochs in each round and the weights were saved after each round)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQC4XYBw1QQD"
      },
      "source": [
        "#Function for creating the dataset\n",
        "def get_dataset(token_list):\n",
        "  inp_token=[i[:-1] for i in token_list]\n",
        "  target_token=[i[1:] for i in token_list]\n",
        "  ragged_inp=tf.ragged.constant(inp_token)\n",
        "  ragged_target=tf.ragged.constant(target_token)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((ragged_inp,ragged_target))\n",
        "  dataset = dataset.batch(128)\n",
        "  dataset = dataset.map(lambda x,y: (x.to_tensor(default_value=0, shape=[None, None]), y.to_tensor(default_value=0, shape=[None, None])), num_parallel_calls=5)\n",
        "  dataset = dataset.prefetch(1)\n",
        "  return dataset\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huwTY3vXNWuf"
      },
      "source": [
        "#Creating the dataset\n",
        "dataset=get_dataset(cleaned_list)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7vIGyclQF4x",
        "outputId": "03096f59-4cc9-4425-e86e-40a4e50a7666"
      },
      "source": [
        "for batch in dataset:\n",
        "  print('Input data:',batch[0])\n",
        "  print('Target data:',batch[1])\n",
        "  break"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data: tf.Tensor(\n",
            "[[   1 3818   49 ...    0    0    0]\n",
            " [   1 3107   34 ...    0    0    0]\n",
            " [   1 4610   93 ...    0    0    0]\n",
            " ...\n",
            " [   1    8 1727 ...    0    0    0]\n",
            " [   1 2692 4675 ...    0    0    0]\n",
            " [   1  130  410 ...    0    0    0]], shape=(128, 147), dtype=int32)\n",
            "Target data: tf.Tensor(\n",
            "[[3818   49   27 ...    0    0    0]\n",
            " [3107   34 3882 ...    0    0    0]\n",
            " [4610   93  576 ...    0    0    0]\n",
            " ...\n",
            " [   8 1727 3501 ...    0    0    0]\n",
            " [2692 4675 5979 ...    0    0    0]\n",
            " [ 130  410 1746 ...    0    0    0]], shape=(128, 147), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MLTf_D4NWl7"
      },
      "source": [
        "#Baseline model function\n",
        "def create_model(embed_dim,vocab_size,hidden_units):\n",
        "  model=Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,mask_zero=True))\n",
        "  model.add(LSTM(units=hidden_units,return_sequences=True))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "  model.add(LSTM(units=hidden_units,return_sequences=True))\n",
        "  model.add(Dropout(rate=0.1))\n",
        "  model.add(TimeDistributed(Dense(units=vocab_size,activation='softmax')))\n",
        "  model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics='accuracy')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY0pccdJVjOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44b4fe0-8c06-4d82-95db-c5d131d3d793"
      },
      "source": [
        "#Creating the model\n",
        "model=create_model(embed_dim=300, vocab_size=6000, hidden_units=512)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 300)         1800000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 512)         1665024   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 512)         2099200   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, None, 6000)        3078000   \n",
            "=================================================================\n",
            "Total params: 8,642,224\n",
            "Trainable params: 8,642,224\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtVmAVdFR65x",
        "outputId": "d590a346-f891-4172-b713-1c7ef5b1b77a"
      },
      "source": [
        "#Training the model\n",
        "hitory=model.fit(dataset,epochs=20,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "472/472 [==============================] - 182s 334ms/step - loss: 1.2355 - accuracy: 0.2815\n",
            "Epoch 2/20\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.2310 - accuracy: 0.2829\n",
            "Epoch 3/20\n",
            "472/472 [==============================] - 158s 334ms/step - loss: 1.2266 - accuracy: 0.2842\n",
            "Epoch 4/20\n",
            "472/472 [==============================] - 158s 334ms/step - loss: 1.2221 - accuracy: 0.2857\n",
            "Epoch 5/20\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.2180 - accuracy: 0.2870\n",
            "Epoch 6/20\n",
            "472/472 [==============================] - 158s 336ms/step - loss: 1.2138 - accuracy: 0.2879\n",
            "Epoch 7/20\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.2096 - accuracy: 0.2892\n",
            "Epoch 8/20\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.2059 - accuracy: 0.2904\n",
            "Epoch 9/20\n",
            "472/472 [==============================] - 155s 329ms/step - loss: 1.2018 - accuracy: 0.2917\n",
            "Epoch 10/20\n",
            "472/472 [==============================] - 155s 328ms/step - loss: 1.1980 - accuracy: 0.2926\n",
            "Epoch 11/20\n",
            "472/472 [==============================] - 154s 327ms/step - loss: 1.1944 - accuracy: 0.2938\n",
            "Epoch 12/20\n",
            "472/472 [==============================] - 155s 329ms/step - loss: 1.1908 - accuracy: 0.2952\n",
            "472/472 [==============================] - 155s 329ms/step - loss: 1.1908 - accuracy: 0.2952\n",
            "Epoch 13/20\n",
            "Epoch 13/20\n",
            "472/472 [==============================] - 155s 329ms/step - loss: 1.1870 - accuracy: 0.2960\n",
            "472/472 [==============================] - 155s 329ms/step - loss: 1.1870 - accuracy: 0.2960\n",
            "Epoch 14/20\n",
            "Epoch 14/20\n",
            "472/472 [==============================] - 158s 334ms/step - loss: 1.1834 - accuracy: 0.2972\n",
            "472/472 [==============================] - 158s 334ms/step - loss: 1.1834 - accuracy: 0.2972\n",
            "Epoch 15/20\n",
            "Epoch 15/20\n",
            "472/472 [==============================] - 159s 336ms/step - loss: 1.1802 - accuracy: 0.2982\n",
            "472/472 [==============================] - 159s 336ms/step - loss: 1.1802 - accuracy: 0.2982\n",
            "Epoch 16/20\n",
            "Epoch 16/20\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1769 - accuracy: 0.2990\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1769 - accuracy: 0.2990\n",
            "Epoch 17/20\n",
            "Epoch 17/20\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1736 - accuracy: 0.3001\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1736 - accuracy: 0.3001\n",
            "Epoch 18/20\n",
            "Epoch 18/20\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1705 - accuracy: 0.3010\n",
            "472/472 [==============================] - 156s 330ms/step - loss: 1.1705 - accuracy: 0.3010\n",
            "Epoch 19/20\n",
            "Epoch 19/20\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.1672 - accuracy: 0.3020\n",
            "472/472 [==============================] - 158s 335ms/step - loss: 1.1672 - accuracy: 0.3020\n",
            "Epoch 20/20\n",
            "Epoch 20/20\n",
            "472/472 [==============================] - 157s 332ms/step - loss: 1.1643 - accuracy: 0.3027\n",
            "472/472 [==============================] - 157s 332ms/step - loss: 1.1643 - accuracy: 0.3027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifiES05fnLzi"
      },
      "source": [
        "#Saving the model for further use\n",
        "model.save('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/baseline1.h5')\n",
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_FxJ7UqscNP"
      },
      "source": [
        "#Loading the saved model\n",
        "from keras.models import load_model\n",
        "model=load_model('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/baseline1.h5')"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW7YJiysjoMu"
      },
      "source": [
        "#Getting the Embedding layer weights for further use\n",
        "embed_weights=model.get_layer(index=0).get_weights()[0]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP0YTSmLuQe1"
      },
      "source": [
        "def predict_text(inp_text):\n",
        "  print('\\033[1m'+'The actual text is:'+'\\033[0m',inp_text)\n",
        "  test_input=texts_to_sequences(encode_data(inp_text))\n",
        "  test_input.append(2)\n",
        "  test_input=[1]+test_input\n",
        "  test_1=test_input[:10]\n",
        "  test_1=[test_1]\n",
        "  test_piece=[idx2word[i] for i in test_1[0]]\n",
        "  print('\\033[1m'+'The input text given to the model:'+'\\033[0m',sp.decode_pieces(test_piece))\n",
        "  for i in range(20):\n",
        "    out=model.predict(test_1)\n",
        "    tok=np.argmax(out[:,-1,:],axis=-1)[0]\n",
        "    if tok==2:\n",
        "      break\n",
        "    else:\n",
        "      test_1[0].append(tok)\n",
        "  print('\\033[1m'+'The text predicted by the model is:'+'\\033[0m',sp.decode_pieces([idx2word[i] for i in test_1[0]]))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZpuoE4YN4BU",
        "outputId": "302736ea-e400-439d-9466-4a549189c37f"
      },
      "source": [
        "for i in np.random.randint(0,18000,5):\n",
        "  print(i)\n",
        "  predict_text(list_of_sentence[i])"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "790\n",
            "\u001b[1mThe actual text is:\u001b[0m although gushi khan had granted the dalai lama supreme authority as goldstein writes the title of 'king of tibet' was conferred upon gushi khan spending his summers in pastures north of lhasa and occupying lhasa each winter.\n",
            "\u001b[1mThe input text given to the model:\u001b[0m although gushi khan had granted the dal\n",
            "\u001b[1mThe text predicted by the model is:\u001b[0m although gushi khan had granted the dalai lama's death in the first half of the 19th century the mongol yuan dynasty was\n",
            "2703\n",
            "\u001b[1mThe actual text is:\u001b[0m the term for anybody who has achieved nirvana including the buddha is arahant.bodhi pali and sanskrit in devanagari  is a term applied to the experience of awakening of arahants.\n",
            "\u001b[1mThe input text given to the model:\u001b[0m the term for anybody who has achieved\n",
            "\u001b[1mThe text predicted by the model is:\u001b[0m the term for anybody who has achieved a significant role in the development of the new york times's first publication in the united states is\n",
            "13729\n",
            "\u001b[1mThe actual text is:\u001b[0m begun in 1879 as a small boarding house the hotel grew through a series of uncoordinated expansions.\n",
            "\u001b[1mThe input text given to the model:\u001b[0m begun in 1879 as a small boarding\n",
            "\u001b[1mThe text predicted by the model is:\u001b[0m begun in 1879 as a small boarding house the city was founded in 1876 and the city of richmond was founded in 1878. the city\n",
            "12180\n",
            "\u001b[1mThe actual text is:\u001b[0m in general it may be stated that such woods of medium growth afford stronger material than when very rapidly or very slowly grown.\n",
            "\u001b[1mThe input text given to the model:\u001b[0m in general it may be stated that such wood\n",
            "\u001b[1mThe text predicted by the model is:\u001b[0m in general it may be stated that such wood is a type of bird species and the most common species of the earth.\n",
            "6350\n",
            "\u001b[1mThe actual text is:\u001b[0m with aspirations to obtain a degree at university college london bell considered his next years as preparation for the degree examinations devoting his spare time at his family's residence to studying.helping his father in visible speech demonstrations and lectures brought bell to susanna e. hull's private school for the deaf in south kensington london.\n",
            "\u001b[1mThe input text given to the model:\u001b[0m with aspirations to obtain a degree at university\n",
            "\u001b[1mThe text predicted by the model is:\u001b[0m with aspirations to obtain a degree at university's public school the university is home to the university of notre dame.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-99_oJGzdNim"
      },
      "source": [
        "From the above model predictions, we can see that the even though the sentences make some sense, the overall accuracy is not that great. But for a baseline language model, it looks okay and we can proceed to the next part on the basis of this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTs8JZeCgv_N"
      },
      "source": [
        "##5. Encoder-Decoder Model for Question Answer Bot\n",
        "Using the above "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "e-eoo3xD_O9Y",
        "outputId": "6624a7e1-edfa-419e-b54e-025188bf134c"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>ans_start</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>269</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>207</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>526</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>166</td>\n",
              "      <td>Houston, Texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>276</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Id    title  ... ans_start                 text\n",
              "0  56be85543aeaaa14008c9063  Beyoncé  ...       269    in the late 1990s\n",
              "1  56be85543aeaaa14008c9065  Beyoncé  ...       207  singing and dancing\n",
              "2  56be85543aeaaa14008c9066  Beyoncé  ...       526                 2003\n",
              "3  56bf6b0f3aeaaa14008c9601  Beyoncé  ...       166       Houston, Texas\n",
              "4  56bf6b0f3aeaaa14008c9602  Beyoncé  ...       276           late 1990s\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt3ofBWc_O6V"
      },
      "source": [
        "question_set=df.question.values\n",
        "answer_set=df.text.values"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-gFaHZ8_O3b"
      },
      "source": [
        "cleaned_question=[re.sub('[^A-Za-z0-9.\\' ]+','',i) for i in question_set]\n",
        "cleaned_question=[i.lower() for i in question_set]\n",
        "cleaned_answer=[re.sub('[^A-Za-z0-9.\\' ]+','',i) for i in answer_set]\n",
        "cleaned_answer=[i.lower() for i in answer_set]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIkT_rU6_O0e"
      },
      "source": [
        "question_tokens=[]\n",
        "answer_tokens=[]\n",
        "for i in cleaned_question:\n",
        "  encoded_data=encode_data(i)\n",
        "  tokens=texts_to_sequences(encoded_data)\n",
        "  tokens.append(2)\n",
        "  tokens=[1]+tokens\n",
        "  question_tokens.append(tokens)\n",
        "for i in cleaned_answer:\n",
        "  encoded_data=encode_data(i)\n",
        "  tokens=texts_to_sequences(encoded_data)\n",
        "  tokens.append(2)\n",
        "  tokens=[1]+tokens\n",
        "  answer_tokens.append(tokens)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXiyML7L_OxY"
      },
      "source": [
        "ragged_ques = tf.ragged.constant(question_tokens)\n",
        "ragged_ans= tf.ragged.constant(answer_tokens)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFQ1RscH_Oux"
      },
      "source": [
        "def process(ques, ans):\n",
        "    in_ans  = ans[:,:-1]\n",
        "    out_ans = ans[:,1:]\n",
        "    \n",
        "    in_ques  = ques.to_tensor(default_value=0, shape=[None, None])\n",
        "    in_ans  = in_ans.to_tensor(default_value=0, shape=[None, None])\n",
        "    out_ans = out_ans.to_tensor(default_value=0, shape=[None, None])\n",
        "    \n",
        "    return (in_ques, in_ans), out_ans\n",
        "dataset = tf.data.Dataset.from_tensor_slices((ragged_ques, ragged_ans))\n",
        "\n",
        "# Specify the batch size \n",
        "dataset = dataset.batch(256)\n",
        "\n",
        "# Use the .map() on the helper function defined above\n",
        "dataset = dataset.map(process, num_parallel_calls=4)\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tajvgiQ_Or0"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "inputs_ques = tf.keras.Input(shape=[None])\n",
        "inputs_ans = tf.keras.Input(shape=[None])\n",
        "\n",
        "emb_eng = tf.keras.layers.Embedding(input_dim=6000, \n",
        "                                    output_dim = 300,\n",
        "                                    weights=[embed_weights],\n",
        "                                    mask_zero=True,\n",
        "                                    trainable=False,\n",
        "                                    name='Encoder_embedding')\n",
        "emb_out=emb_eng(inputs_ques)\n",
        "encoder_rnn = tf.keras.layers.GRU(256, return_sequences=True, name='Encoder_RNN_f')\n",
        "encoder_states = encoder_rnn(emb_out)\n",
        "\n",
        "                                    \n",
        "last_encoder_state = encoder_states[:,-1,:]\n",
        "\n",
        "# Decoder\n",
        "emb_ans = tf.keras.layers.Embedding(input_dim=6000, \n",
        "                                    output_dim = 300,\n",
        "                                    weights=[embed_weights],\n",
        "                                    mask_zero=True,\n",
        "                                    trainable=False,\n",
        "                                    name='Decoder_embedding')(inputs_ans)\n",
        "\n",
        "\n",
        "decoder_rnn = tf.keras.layers.GRU(256, return_sequences=True,  name='Decoder_RNN')\n",
        "decoder_states = decoder_rnn(emb_ans, initial_state=last_encoder_state)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZDapERT_OpU"
      },
      "source": [
        "hidden_size=64\n",
        "\n",
        "dense_projection_e = tf.keras.layers.Dense(hidden_size, activation=None, use_bias=False)\n",
        "dense_projection_d = tf.keras.layers.Dense(hidden_size, activation=None, use_bias=False)\n",
        "\n",
        "r_encoder = dense_projection_e(encoder_states)\n",
        "r_decoder = dense_projection_d(decoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHzKlIx_Ol-"
      },
      "source": [
        "# Step 1:\n",
        "# Expand the dimensions for the query and the value at the appropriate locations \n",
        "query = tf.expand_dims(r_decoder, axis=2)\n",
        "value = tf.expand_dims(r_encoder, axis=1)\n",
        "\n",
        "\n",
        "# Step 2:\n",
        "# #Compute the score as in Bahdanau's paper  \n",
        "score = tf.reduce_sum(tf.tanh(query + value, name='Score'), axis=-1)\n",
        "\n",
        "\n",
        "# Additional step:\n",
        "# It is crucial to manually mask the 'padded' hidden states otherwise there will be a mismatch between the encoder-decoder model defined above\n",
        "# And the attention weights computed below \n",
        "N_query = tf.reduce_sum(tf.cast(inputs_ans>0, tf.float32), axis=1)\n",
        "query_mask = tf.sequence_mask(N_query)\n",
        "\n",
        "N_value = tf.reduce_sum(tf.cast(inputs_ques>0, tf.float32), axis=1)\n",
        "value_mask = tf.sequence_mask(N_value)\n",
        "# # Mask the score\n",
        "m1 = tf.sequence_mask(N_query, dtype=tf.float32)\n",
        "m2 = tf.sequence_mask(N_value, dtype=tf.float32)\n",
        "\n",
        "score = tf.transpose(tf.multiply(tf.transpose(score,[2,0,1]), m1),[1,2,0])\n",
        "score = tf.transpose(tf.multiply(tf.transpose(score,[1,0,2]), m2), [1,0,2])\n",
        "\n",
        "# Step 3: Use the scores to get a distribution of probabilities\n",
        "# Hint: Use softmax \n",
        "weights = tf.nn.softmax(score, axis=2, name='Weights')\n",
        "\n",
        "# Step 4: Get the context vector by multiplying the encoder states with the weights \n",
        "# # Compute the context\n",
        "context_vector = tf.matmul(weights, encoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVvzmca8Lwp1"
      },
      "source": [
        "enhanced_encoding = tf.keras.layers.Concatenate(axis=2)([context_vector, decoder_states])\n",
        "\n",
        "# Pass the enhanced encoding to a dense layer with softmax activation \n",
        "output_ans = tf.keras.layers.Dense(6000,activation='softmax')(enhanced_encoding)\n",
        "\n",
        "\n",
        "# Set up the model with appropriate inputs and the output defined above \n",
        "model2 = tf.keras.Model(inputs=(inputs_ques, inputs_ans), outputs=output_ans)\n",
        "\n",
        "# Choose an appropriate learning rate and optimizer\n",
        "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPaq4djdLwm4",
        "outputId": "f9d1067d-2521-4ef8-8c9c-7a7a3c0067a1"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_embedding (Embedding)   (None, None, 300)    1800000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_RNN_f (GRU)             (None, None, 256)    428544      Encoder_embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_embedding (Embedding)   (None, None, 300)    1800000     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 256)          0           Encoder_RNN_f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_RNN (GRU)               (None, None, 256)    428544      Decoder_embedding[0][0]          \n",
            "                                                                 tf.__operators__.getitem[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 64)     16384       Decoder_RNN[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 64)     16384       Encoder_RNN_f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims (TFOpLambda)     (None, None, 1, 64)  0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_1 (TFOpLambda)   (None, 1, None, 64)  0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (None, None, None, 6 0           tf.expand_dims[0][0]             \n",
            "                                                                 tf.expand_dims_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.greater (TFOpLambda)    (None, None)         0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.tanh (TFOpLambda)       (None, None, None, 6 0           tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.cast (TFOpLambda)            (None, None)         0           tf.math.greater[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum (TFOpLambda) (None, None, None)   0           tf.math.tanh[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum_1 (TFOpLambd (None,)              0           tf.cast[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose (TFOpLam (None, None, None)   0           tf.math.reduce_sum[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.sequence_mask_2 (TFOpLambda) (None, None)         0           tf.math.reduce_sum_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.greater_1 (TFOpLambda)  (None, None)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply (TFOpLambda)   (None, None, None)   0           tf.compat.v1.transpose[0][0]     \n",
            "                                                                 tf.sequence_mask_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.cast_1 (TFOpLambda)          (None, None)         0           tf.math.greater_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_1 (TFOpL (None, None, None)   0           tf.math.multiply[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum_2 (TFOpLambd (None,)              0           tf.cast_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_2 (TFOpL (None, None, None)   0           tf.compat.v1.transpose_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.sequence_mask_3 (TFOpLambda) (None, None)         0           tf.math.reduce_sum_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply_1 (TFOpLambda) (None, None, None)   0           tf.compat.v1.transpose_2[0][0]   \n",
            "                                                                 tf.sequence_mask_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_3 (TFOpL (None, None, None)   0           tf.math.multiply_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.softmax (TFOpLambda)      (None, None, None)   0           tf.compat.v1.transpose_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul (TFOpLambda)   (None, None, 256)    0           tf.nn.softmax[0][0]              \n",
            "                                                                 Encoder_RNN_f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, None, 512)    0           tf.linalg.matmul[0][0]           \n",
            "                                                                 Decoder_RNN[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 6000)   3078000     concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 7,567,856\n",
            "Trainable params: 3,967,856\n",
            "Non-trainable params: 3,600,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXXHiX43MQdH",
        "outputId": "c3569941-e818-4287-861f-fc78d46730c9"
      },
      "source": [
        "history2=model2.fit(dataset,epochs=20,verbose=1)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "340/340 [==============================] - 22s 65ms/step - loss: 0.4404\n",
            "Epoch 2/20\n",
            "340/340 [==============================] - 22s 65ms/step - loss: 0.4347\n",
            "Epoch 3/20\n",
            "340/340 [==============================] - 23s 66ms/step - loss: 0.4291\n",
            "Epoch 4/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4246\n",
            "Epoch 5/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4216\n",
            "Epoch 6/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4171\n",
            "Epoch 7/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4128\n",
            "Epoch 8/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4093\n",
            "Epoch 9/20\n",
            "340/340 [==============================] - 23s 68ms/step - loss: 0.4069\n",
            "Epoch 10/20\n",
            "340/340 [==============================] - 23s 67ms/step - loss: 0.4035\n",
            "Epoch 11/20\n",
            "340/340 [==============================] - 23s 68ms/step - loss: 0.4010\n",
            "Epoch 12/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.4024\n",
            "Epoch 13/20\n",
            "340/340 [==============================] - 23s 68ms/step - loss: 0.3966\n",
            "Epoch 14/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.3928\n",
            "Epoch 15/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.3904\n",
            "Epoch 16/20\n",
            "340/340 [==============================] - 23s 68ms/step - loss: 0.3887\n",
            "Epoch 17/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.3846\n",
            "Epoch 18/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.3813\n",
            "Epoch 19/20\n",
            "340/340 [==============================] - 23s 68ms/step - loss: 0.3775\n",
            "Epoch 20/20\n",
            "340/340 [==============================] - 23s 69ms/step - loss: 0.3739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7qNga6Ns60b"
      },
      "source": [
        "test_ques=df.question.iloc[15]\n",
        "test_ques=texts_to_sequences(encode_data(test_ques))\n",
        "test_ques.append(2)\n",
        "test_ques=[1]+test_ques\n",
        "test_ques=[test_ques]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CfBjbqPPSGQ",
        "outputId": "eabdef7f-e614-4354-87cc-e12ade46dd3f"
      },
      "source": [
        "print([idx2word[i] for i in test_ques[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', '▁', '<unk>', 'f', 'ter', '▁her', '▁second', '▁sol', 'o', '▁album', '<unk>', '▁what', '▁other', '▁entertainment', '▁vent', 'ure', '▁did', '▁', '<unk>', 'ey', 'once', '▁expl', 'ore', '<unk>', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A45q-w8RsDt4"
      },
      "source": [
        "model2.save('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/encode.h5')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1UYBrW1Qls2"
      },
      "source": [
        "from keras.models import load_model\n",
        "model2=load_model('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/encode.h5')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVo5-35Ki1di"
      },
      "source": [
        "def evaluate(test_ques):\n",
        "  print(test_ques)\n",
        "  sentence = texts_to_sequences(encode_data(test_ques))\n",
        "  START_TOKEN=[word2idx['<s>']]\n",
        "  END_TOKEN=[word2idx['</s>']]\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + sentence + END_TOKEN, axis=0)\n",
        "  output = tf.expand_dims(START_TOKEN, 0)\n",
        "  for i in range(20):\n",
        "    predictions = model2(inputs=[sentence, output], training=False)\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # concatenated the predicted_id to the output which is given to the decoder as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)\n",
        "\n",
        "def predict(sentence):\n",
        "  prediction = evaluate(sentence)\n",
        "  predicted_sentence = [idx2word[i] for i in prediction.numpy()]\n",
        "  return predicted_sentence"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKJfR0smjIJX",
        "outputId": "ffacd709-ada3-458c-a566-879052ee1c37"
      },
      "source": [
        "for i in range(0,100,5):\n",
        "  print(predict(df.question.iloc[i]))\n",
        "  print(df.text.iloc[i])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When did Beyonce start becoming popular?\n",
            "['<s>', '▁n', '<unk>', 's', '▁n', 'ied', 'er', 'ts']\n",
            "in the late 1990s\n",
            "In what R&B group was she the lead singer?\n",
            "['<s>', '▁african', '▁american', '▁tribes']\n",
            "Destiny's Child\n",
            "What was the first album Beyoncé released as a solo artist?\n",
            "['<s>', '▁the', '▁tr', 'ips']\n",
            "Dangerously in Love\n",
            "After her second solo album, what other entertainment venture did Beyonce explore?\n",
            "['<s>', '▁inf', 'in', 'ite']\n",
            "acting\n",
            "Which album was darker in tone from her previous work?\n",
            "['<s>', '▁aristot', 'ians']\n",
            "Beyoncé\n",
            "Who is Beyoncé married to?\n",
            "['<s>', '▁engine', 'er']\n",
            "Jay Z\n",
            "In which decade did the Recording Industry Association of America recognize Beyonce as the The Top Certified Artist?\n",
            "['<s>', '▁early', '▁1990', 's']\n",
            "2000s\n",
            "How many records did Beyoncé sell as part of Destiny's Child?\n",
            "['<s>', '▁the', '▁su', 'e', 'z', '▁canal']\n",
            "60 million\n",
            "Where did Beyonce get her name from?\n",
            "['<s>', '▁to', '▁use', '▁a', '▁th', 'ing', '<unk>', '▁sal', 't', '<unk>', 'day', '▁his', '▁case']\n",
            "her mother's maiden name\n",
            "What younger sister of Beyonce also appeared in Destiny's Child?\n",
            "['<s>', '▁most', '▁widespread']\n",
            "Solange\n",
            "Beyoncé is a descendant of which Acadian leader?\n",
            "['<s>', '▁sh', 'ir', 'rel']\n",
            "Joseph Broussard.\n",
            "Which of her teachers discovered Beyonce's musical talent?\n",
            "['<s>', '▁cap', 'ric', 'ain']\n",
            "dance instructor Darlette Johnson\n",
            "What was the name of Beyoncé's first dance instructor?\n",
            "['<s>', '▁john', '▁i', \"'\", 's', '▁still', '▁be', '▁pred', 'icted']\n",
            "Darlette Johnson\n",
            "Who was the first record label to give the girls a record deal?\n",
            "['<s>', '▁sym', 'ph', 'ony']\n",
            "Elektra Records\n",
            "At what age did Beyonce meet LaTavia Robertson?\n",
            "['<s>', '▁5', '<unk>']\n",
            "age eight\n",
            "Who signed the girl group on October 5, 1995?\n",
            "['<s>', '▁super', 'com', 'mon', '▁protection', '▁service']\n",
            "Dwayne Wiggins's Grass Roots Entertainment\n",
            "The name Destiny's Child was based on a quote in which book of the Bible?\n",
            "['<s>', '▁w', 'es']\n",
            "Book of Isaiah\n",
            "Destiny's Child song, Killing Time, was included in which film's soundtrack?\n",
            "['<s>', '▁rob', 'et', '▁bar', 'rel', 's']\n",
            "Men in Black.\n",
            "What event occured after she was publicly criticized?\n",
            "['<s>', '▁an', '▁threat', 's']\n",
            "boyfriend left her\n",
            "Who replaced Luckett and Roberson in Destiny's Child?\n",
            "['<s>', '▁mar', 's']\n",
            "Farrah Franklin and Michelle Williams.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaa2uWh_jIFn"
      },
      "source": [
        "import pydot\n",
        "from tensorflow.keras.utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "SVG(model_to_dot(model2,  show_shapes=True, show_layer_names=True, dpi=52, rankdir='TB').create_svg())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}