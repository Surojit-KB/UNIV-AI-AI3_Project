{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPplSwOPzJ8OqkIin6nLT4y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surojit-KB/UNIV-AI-AI3_Project/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ2WxVPn06oG"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Embedding, Dense, TimeDistributed,LSTM, GRU,Dropout, Bidirectional, Conv1D, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmc7WcDt_gx4",
        "outputId": "da4092b5-0ee0-47c4-9af0-ddbfccb95e7b"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 9.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akjeZrq1_kg6"
      },
      "source": [
        "import sentencepiece as spm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kji07lM1QdY",
        "outputId": "e3a6c068-3812-4a28-aebc-e71020f54426"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-sJeZUHIzc_"
      },
      "source": [
        "def get_dataframe(file):\n",
        "  f = open(file, 'r')\n",
        "  #loading json file \n",
        "  data = json.loads(f.read())\n",
        "  #creating empty lists to store df values \n",
        "  iid = []\n",
        "  tit = []\n",
        "  con = []\n",
        "  que = []\n",
        "  ans = []\n",
        "  txt = []\n",
        "  #root tags contains 'title' tag and 'paragraphs' list \n",
        "  for i in range(len(data['data'])):\n",
        "    title = data['data'][i]['title']\n",
        "    #'paragraphs' list contains 'context' tag and 'qas' list \n",
        "    for p in range(len(data['data'][i]['paragraphs'])):\n",
        "      context = data['data'][i]['paragraphs'][p]['context']\n",
        "      for q in range(len(data['data'][i]['paragraphs'][p]['qas'])):\n",
        "        # 'qas'list contains 'question', 'Id' tag and 'answers' list \n",
        "        question = data['data'][i]['paragraphs'][p]['qas'][q]['question']\n",
        "        id = data['data'][i]['paragraphs'][p]['qas'][q]['id']\n",
        "        #'answers' list contains 'answer_start' and 'text' tag \n",
        "        for a in range(len(data['data'][i]['paragraphs'][p]['qas'][q]['answers'])):\n",
        "          ans_start = data['data'][i]['paragraphs'][p]['qas'][q]['answers'][a]['answer_start']\n",
        "          text = data['data'][i]['paragraphs'][p]['qas'][q]['answers'][a]['text']\n",
        "          \n",
        "          #appending values to list \n",
        "          iid.append(id)\n",
        "          tit.append(title)\n",
        "          con.append(context)\n",
        "          que.append(question)\n",
        "          ans.append(ans_start)\n",
        "          txt.append(text)\n",
        "  #creating dataframe from lists \n",
        "  new_df = pd.DataFrame(columns=['Id', 'title', 'context', 'question', 'ans_start', 'text'])\n",
        "  new_df.Id = iid\n",
        "  new_df.title=tit\n",
        "  new_df.context = con\n",
        "  new_df.question = que\n",
        "  new_df.ans_start = ans \n",
        "  new_df.text = txt \n",
        "  #removing duplicate columns \n",
        "  final_df = new_df.drop_duplicates(keep='first')\n",
        "\n",
        "  return final_df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVjCaABR1QbA"
      },
      "source": [
        "df=get_dataframe('/content/drive/MyDrive/Colab Notebooks/UNIV-AI-AI3/train-v2.0.json')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lILTaiexBCHh",
        "outputId": "91722ca0-78a6-4813-accf-0a1fe1d7e14e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>ans_start</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>269</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>207</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>526</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>166</td>\n",
              "      <td>Houston, Texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>276</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Id    title  ... ans_start                 text\n",
              "0  56be85543aeaaa14008c9063  Beyoncé  ...       269    in the late 1990s\n",
              "1  56be85543aeaaa14008c9065  Beyoncé  ...       207  singing and dancing\n",
              "2  56be85543aeaaa14008c9066  Beyoncé  ...       526                 2003\n",
              "3  56bf6b0f3aeaaa14008c9601  Beyoncé  ...       166       Houston, Texas\n",
              "4  56bf6b0f3aeaaa14008c9602  Beyoncé  ...       276           late 1990s\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg2GLiuNQ6-o"
      },
      "source": [
        "text=''.join(df.context.unique())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wpzdhSpJCD6"
      },
      "source": [
        "import unicodedata\n",
        "normalized_text=str(unicodedata.normalize('NFD', text).encode('ascii', 'ignore'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR14yJDq-QPK"
      },
      "source": [
        "cleaned_text=re.sub('[^A-Za-z0-9.\\' ]+','',normalized_text[2:])\n",
        "cleaned_text=cleaned_text.lower()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diLgwcQPUcMR"
      },
      "source": [
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "websites = \"[.](com|net|org|io|gov)\"\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    return sentences"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHxhlie2Vl68",
        "outputId": "f0a96f74-9047-4316-da7f-da616be02116"
      },
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLEbvGPeVwVj"
      },
      "source": [
        "list_of_sentence=tokenizer.tokenize(cleaned_text)\n",
        "#list_of_sentence=[\"<s> \"+i+\" </s>\" for i in list_of_sentence]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HfTBSgMNZBL",
        "outputId": "c7f5b3bc-09af-46d4-e10e-d2f6c2de4841"
      },
      "source": [
        "list_of_sentence[:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['beyonce giselle knowlescarter bijnse beeyonsay born september 4 1981 is an american singer songwriter record producer and actress.',\n",
              " \"born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of rb girlgroup destiny's child.\",\n",
              " \"managed by her father mathew knowles the group became one of the world's bestselling girl groups of all time.\",\n",
              " \"their hiatus saw the release of beyonce's debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 numberone singles crazy in love and baby boy.following the disbandment of destiny's child in june 2005 she released her second solo album b'day 2006 which contained hits deja vu irreplaceable and beautiful liar.\",\n",
              " 'beyonce also ventured into acting with a golden globenominated performance in dreamgirls 2006 and starring roles in the pink panther 2006 and obsessed 2009. her marriage to rapper jay z and portrayal of etta james in cadillac records 2008 influenced her third album i am... sasha fierce 2008 which saw the birth of her alterego sasha fierce and earned a recordsetting six grammy awards in 2010 including song of the year for single ladies put a ring on it.',\n",
              " 'beyonce took a hiatus from music in 2010 and took over management of her career her fourth album 4 2011 was subsequently mellower in tone exploring 1970s funk 1980s pop and 1990s soul.',\n",
              " 'her critically acclaimed fifth studio album beyonce 2013 was distinguished from previous releases by its experimental production and exploration of darker themes.a selfdescribed modernday feminist beyonce creates songs that are often characterized by themes of love relationships and monogamy as well as female sexuality and empowerment.',\n",
              " 'on stage her dynamic highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music.',\n",
              " \"throughout a career spanning 19 years she has sold over 118 million records as a solo artist and a further 60 million with destiny's child making her one of the bestselling music artists of all time.\",\n",
              " \"she has won 20 grammy awards and is the most nominated woman in the award's history.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoFkXlsY__v3"
      },
      "source": [
        "rect_path='./context.txt'\n",
        "with open(rect_path, 'w', errors = 'ignore') as file:\n",
        "    for line in list_of_sentence:\n",
        "      file.write(line+'\\n')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn8H52BlJaLf"
      },
      "source": [
        "vocab_size=5000\n",
        "tokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size,lower=True,oov_token='UNK')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDs0AJn--cX"
      },
      "source": [
        "def sentencepiece_():\n",
        "  templates= '--input={} \\\n",
        "  --pad_id={} \\\n",
        "  --bos_id={} \\\n",
        "  --eos_id={} \\\n",
        "  --unk_id={} \\\n",
        "  --model_prefix={} \\\n",
        "  --vocab_size={} \\\n",
        "  --character_coverage={} \\\n",
        "  --model_type={}'\n",
        "\n",
        "  train_input_file = rect_path\n",
        "  prefix = './en_spm' \n",
        "  pad_id=0  \n",
        "  vocab_size = 6000 \n",
        "  bos_id=1 \n",
        "  eos_id=2 \n",
        "  unk_id=3 \n",
        "  character_coverage = 1.0 \n",
        "  model_type ='bpe' \n",
        "\n",
        "\n",
        "  cmd = templates.format(train_input_file,\n",
        "                pad_id,\n",
        "                bos_id,\n",
        "                eos_id,\n",
        "                unk_id,\n",
        "                prefix,\n",
        "                vocab_size,\n",
        "                character_coverage,\n",
        "                model_type)\n",
        "  # `m.vocab` is just a reference. not used in the segmentation.\n",
        "  spm.SentencePieceTrainer.train(cmd)\n",
        "\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  sp.load('./en_spm.model')\n",
        "  return sp"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJsV7uAnApIt"
      },
      "source": [
        "sp=sentencepiece_()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXwXLZdo-L6q"
      },
      "source": [
        "def dictionary():\n",
        "  with open('./en_spm.vocab', encoding='utf-8') as f:\n",
        "      Vo = [doc.strip().split(\"\\t\") for doc in f]\n",
        "\n",
        "  # w[0]: token name    \n",
        "  # w[1]: token score \n",
        "  word2idx_en = {w[0]: i for i, w in enumerate(Vo)}\n",
        "  idx2word_en = {i:w[0] for i, w in enumerate(Vo)}\n",
        "\n",
        "  return word2idx_en, idx2word_en"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjYYcY1D-Lxh"
      },
      "source": [
        "word2idx,idx2word=dictionary()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5jGS5m-CcE7"
      },
      "source": [
        "def encode_data(input):\n",
        "            pieces = sp.encode_as_pieces(input)\n",
        "            return pieces\n",
        "def texts_to_sequences(texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            text_list = text.split()\n",
        "            sequence = [int(word2idx.get(token, word2idx[\"<unk>\"])) for token in text_list]\n",
        "            sequences += sequence\n",
        "        return sequences"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFf5UNv7C1Vl"
      },
      "source": [
        "token_list=[]\n",
        "for i in list_of_sentence:\n",
        "  encoded_data=encode_data(i)\n",
        "  tokens=texts_to_sequences(encoded_data)\n",
        "  tokens.append(2)\n",
        "  tokens=[1]+tokens\n",
        "  token_list.append(tokens)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T32Qa4mLMjyY"
      },
      "source": [
        "cleaned_list=[]\n",
        "for i in token_list:\n",
        "  if len(i)>50 and len(i)<500:\n",
        "    cleaned_list.append(i)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "kUs99K-8QINY",
        "outputId": "1264a6a2-5c08-4688-c289-d14c793a181b"
      },
      "source": [
        "' '.join(idx2word[i] for i in token_list[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<s> ▁beyonce ▁g is el le ▁know les car ter ▁bi j n se ▁be ey ons ay ▁born ▁september ▁4 ▁198 1 ▁is ▁an ▁american ▁sing er ▁song writ er ▁record ▁producer ▁and ▁act ress . </s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq1bDjmwo0BW",
        "outputId": "9b8d0dc7-6771-44aa-9b4a-2166e3324d67"
      },
      "source": [
        "word2idx"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<pad>': 0,\n",
              " '<s>': 1,\n",
              " '</s>': 2,\n",
              " '<unk>': 3,\n",
              " '▁t': 4,\n",
              " '▁a': 5,\n",
              " 'he': 6,\n",
              " 'in': 7,\n",
              " '▁the': 8,\n",
              " 'er': 9,\n",
              " 'on': 10,\n",
              " 're': 11,\n",
              " '▁s': 12,\n",
              " '▁o': 13,\n",
              " '▁c': 14,\n",
              " 'at': 15,\n",
              " 'en': 16,\n",
              " 'an': 17,\n",
              " 'ed': 18,\n",
              " '▁w': 19,\n",
              " 'it': 20,\n",
              " '▁p': 21,\n",
              " 'es': 22,\n",
              " '▁b': 23,\n",
              " '▁in': 24,\n",
              " '▁of': 25,\n",
              " 'al': 26,\n",
              " 'is': 27,\n",
              " 'or': 28,\n",
              " '▁an': 29,\n",
              " '▁f': 30,\n",
              " '▁m': 31,\n",
              " 'ar': 32,\n",
              " 'ic': 33,\n",
              " '▁and': 34,\n",
              " '▁d': 35,\n",
              " 'ion': 36,\n",
              " 'ing': 37,\n",
              " 'ro': 38,\n",
              " 'as': 39,\n",
              " '▁to': 40,\n",
              " '▁h': 41,\n",
              " '▁l': 42,\n",
              " 'ent': 43,\n",
              " '▁e': 44,\n",
              " '▁n': 45,\n",
              " 'ou': 46,\n",
              " '▁re': 47,\n",
              " 'le': 48,\n",
              " '▁g': 49,\n",
              " '▁th': 50,\n",
              " 'om': 51,\n",
              " 'st': 52,\n",
              " 'ly': 53,\n",
              " 'ct': 54,\n",
              " 'il': 55,\n",
              " '▁u': 56,\n",
              " 'ol': 57,\n",
              " 'ation': 58,\n",
              " '▁be': 59,\n",
              " '▁1': 60,\n",
              " 've': 61,\n",
              " 'ch': 62,\n",
              " 'ur': 63,\n",
              " '▁as': 64,\n",
              " 'ce': 65,\n",
              " 'ad': 66,\n",
              " 'am': 67,\n",
              " '▁st': 68,\n",
              " 'id': 69,\n",
              " 'ver': 70,\n",
              " 'im': 71,\n",
              " '▁is': 72,\n",
              " 'ig': 73,\n",
              " '▁for': 74,\n",
              " 'th': 75,\n",
              " 'se': 76,\n",
              " 'ir': 77,\n",
              " '▁on': 78,\n",
              " 'ter': 79,\n",
              " '▁con': 80,\n",
              " '▁de': 81,\n",
              " 'ow': 82,\n",
              " 'ut': 83,\n",
              " 'ul': 84,\n",
              " '▁al': 85,\n",
              " '▁r': 86,\n",
              " '▁was': 87,\n",
              " 'ith': 88,\n",
              " 'ot': 89,\n",
              " 'et': 90,\n",
              " '▁wh': 91,\n",
              " '▁with': 92,\n",
              " '▁by': 93,\n",
              " 'her': 94,\n",
              " 'ist': 95,\n",
              " 'est': 96,\n",
              " '▁it': 97,\n",
              " '▁that': 98,\n",
              " 'us': 99,\n",
              " 'el': 100,\n",
              " 'op': 101,\n",
              " 'rom': 102,\n",
              " 'ay': 103,\n",
              " '▁su': 104,\n",
              " '▁or': 105,\n",
              " '▁v': 106,\n",
              " '▁un': 107,\n",
              " 'ity': 108,\n",
              " '▁com': 109,\n",
              " '▁ch': 110,\n",
              " 'ies': 111,\n",
              " 'and': 112,\n",
              " 'ain': 113,\n",
              " '▁2': 114,\n",
              " '▁pro': 115,\n",
              " '▁mo': 116,\n",
              " 'un': 117,\n",
              " '▁se': 118,\n",
              " 'ate': 119,\n",
              " 'ac': 120,\n",
              " 'ment': 121,\n",
              " 'ers': 122,\n",
              " '▁are': 123,\n",
              " 'ag': 124,\n",
              " '▁k': 125,\n",
              " '▁j': 126,\n",
              " 'if': 127,\n",
              " 'um': 128,\n",
              " 'ab': 129,\n",
              " '▁at': 130,\n",
              " '▁ex': 131,\n",
              " 'ian': 132,\n",
              " 'iv': 133,\n",
              " '▁from': 134,\n",
              " '▁19': 135,\n",
              " 'res': 136,\n",
              " '▁ne': 137,\n",
              " 'em': 138,\n",
              " '▁he': 139,\n",
              " 'pe': 140,\n",
              " 'ial': 141,\n",
              " 'oun': 142,\n",
              " 'ud': 143,\n",
              " 'os': 144,\n",
              " '00': 145,\n",
              " 'qu': 146,\n",
              " 'od': 147,\n",
              " 'ich': 148,\n",
              " 'oc': 149,\n",
              " 'ap': 150,\n",
              " 'ive': 151,\n",
              " 'ort': 152,\n",
              " 'ant': 153,\n",
              " 'ld': 154,\n",
              " 'ia': 155,\n",
              " '▁ar': 156,\n",
              " 'ere': 157,\n",
              " 'ical': 158,\n",
              " '▁tr': 159,\n",
              " 'art': 160,\n",
              " 'ated': 161,\n",
              " 'ib': 162,\n",
              " '▁en': 163,\n",
              " 'igh': 164,\n",
              " 'ill': 165,\n",
              " '▁were': 166,\n",
              " 'ess': 167,\n",
              " '▁which': 168,\n",
              " 'ip': 169,\n",
              " 'all': 170,\n",
              " '▁le': 171,\n",
              " 'ther': 172,\n",
              " 'gh': 173,\n",
              " '▁y': 174,\n",
              " 'der': 175,\n",
              " 'cl': 176,\n",
              " 'ge': 177,\n",
              " 'ast': 178,\n",
              " 'ave': 179,\n",
              " 'ure': 180,\n",
              " 'ary': 181,\n",
              " 'ak': 182,\n",
              " 'ard': 183,\n",
              " 'pp': 184,\n",
              " 'act': 185,\n",
              " '▁wor': 186,\n",
              " 'ions': 187,\n",
              " '▁not': 188,\n",
              " 'ous': 189,\n",
              " 'ect': 190,\n",
              " 'our': 191,\n",
              " '▁comp': 192,\n",
              " 'og': 193,\n",
              " '▁pl': 194,\n",
              " '▁pr': 195,\n",
              " '▁us': 196,\n",
              " '▁ad': 197,\n",
              " 'ish': 198,\n",
              " 'ome': 199,\n",
              " '▁cont': 200,\n",
              " 'per': 201,\n",
              " 'so': 202,\n",
              " 'pt': 203,\n",
              " 'ong': 204,\n",
              " '▁this': 205,\n",
              " 'ber': 206,\n",
              " 'ans': 207,\n",
              " '▁sh': 208,\n",
              " 'du': 209,\n",
              " 'ire': 210,\n",
              " '▁his': 211,\n",
              " 'iz': 212,\n",
              " 'ub': 213,\n",
              " 'ame': 214,\n",
              " '▁cl': 215,\n",
              " 'av': 216,\n",
              " 'cc': 217,\n",
              " 'ations': 218,\n",
              " '▁af': 219,\n",
              " 'ult': 220,\n",
              " '▁int': 221,\n",
              " '▁ro': 222,\n",
              " 'ie': 223,\n",
              " 'rit': 224,\n",
              " 'end': 225,\n",
              " '▁has': 226,\n",
              " '▁also': 227,\n",
              " '▁man': 228,\n",
              " 'ight': 229,\n",
              " '▁per': 230,\n",
              " 'ine': 231,\n",
              " '▁cent': 232,\n",
              " '▁am': 233,\n",
              " 'the': 234,\n",
              " '▁comm': 235,\n",
              " '▁have': 236,\n",
              " '▁other': 237,\n",
              " '▁all': 238,\n",
              " '▁its': 239,\n",
              " 'ide': 240,\n",
              " '▁part': 241,\n",
              " '▁their': 242,\n",
              " 'ass': 243,\n",
              " 'ence': 244,\n",
              " 'age': 245,\n",
              " '▁res': 246,\n",
              " '▁sp': 247,\n",
              " '▁im': 248,\n",
              " 'ally': 249,\n",
              " 'ore': 250,\n",
              " 'rou': 251,\n",
              " 'ign': 252,\n",
              " '▁ab': 253,\n",
              " '▁had': 254,\n",
              " 'ates': 255,\n",
              " 'ell': 256,\n",
              " 'ree': 257,\n",
              " '▁new': 258,\n",
              " 'over': 259,\n",
              " '▁whe': 260,\n",
              " 'ount': 261,\n",
              " 'tern': 262,\n",
              " 'clud': 263,\n",
              " '▁em': 264,\n",
              " 'ang': 265,\n",
              " 'ren': 266,\n",
              " 'ition': 267,\n",
              " 'ry': 268,\n",
              " '▁20': 269,\n",
              " '▁but': 270,\n",
              " 'ru': 271,\n",
              " '▁bo': 272,\n",
              " 'fer': 273,\n",
              " 'ance': 274,\n",
              " '▁200': 275,\n",
              " '▁te': 276,\n",
              " '▁ind': 277,\n",
              " 'ear': 278,\n",
              " '▁one': 279,\n",
              " '▁fir': 280,\n",
              " 'ater': 281,\n",
              " '▁includ': 282,\n",
              " 'red': 283,\n",
              " 'ue': 284,\n",
              " '▁dis': 285,\n",
              " '▁me': 286,\n",
              " 'iver': 287,\n",
              " '▁bec': 288,\n",
              " '▁such': 289,\n",
              " 'ack': 290,\n",
              " '▁can': 291,\n",
              " 'ord': 292,\n",
              " 'own': 293,\n",
              " 'ra': 294,\n",
              " '▁form': 295,\n",
              " 'pl': 296,\n",
              " '▁most': 297,\n",
              " '▁first': 298,\n",
              " 'vel': 299,\n",
              " '▁col': 300,\n",
              " '▁sc': 301,\n",
              " 'lect': 302,\n",
              " 'ould': 303,\n",
              " '▁city': 304,\n",
              " 'con': 305,\n",
              " '▁ag': 306,\n",
              " 'ust': 307,\n",
              " '▁they': 308,\n",
              " 'erm': 309,\n",
              " '▁18': 310,\n",
              " 'out': 311,\n",
              " 'ime': 312,\n",
              " 'ric': 313,\n",
              " '▁sy': 314,\n",
              " 'uring': 315,\n",
              " '▁mar': 316,\n",
              " '▁gen': 317,\n",
              " 'lic': 318,\n",
              " '▁who': 319,\n",
              " 'ational': 320,\n",
              " 'ice': 321,\n",
              " 'hed': 322,\n",
              " 'land': 323,\n",
              " '▁more': 324,\n",
              " '▁been': 325,\n",
              " 'und': 326,\n",
              " '▁some': 327,\n",
              " 'orm': 328,\n",
              " '▁over': 329,\n",
              " 'ile': 330,\n",
              " 'ied': 331,\n",
              " 'ited': 332,\n",
              " '▁rec': 333,\n",
              " 'ces': 334,\n",
              " '▁rel': 335,\n",
              " 'ace': 336,\n",
              " 'een': 337,\n",
              " '▁spe': 338,\n",
              " 'able': 339,\n",
              " '▁ev': 340,\n",
              " '▁reg': 341,\n",
              " 'ced': 342,\n",
              " 'ough': 343,\n",
              " 'port': 344,\n",
              " 'ism': 345,\n",
              " 'ory': 346,\n",
              " '▁3': 347,\n",
              " '▁pop': 348,\n",
              " '▁tw': 349,\n",
              " '▁pol': 350,\n",
              " '▁ph': 351,\n",
              " '▁year': 352,\n",
              " '▁pe': 353,\n",
              " 'ke': 354,\n",
              " 'ens': 355,\n",
              " '▁ma': 356,\n",
              " '▁201': 357,\n",
              " 'ost': 358,\n",
              " '▁cons': 359,\n",
              " 'ound': 360,\n",
              " '▁pres': 361,\n",
              " 'ts': 362,\n",
              " 'ff': 363,\n",
              " 'ors': 364,\n",
              " '▁pre': 365,\n",
              " '▁des': 366,\n",
              " '▁war': 367,\n",
              " 'tw': 368,\n",
              " '▁lar': 369,\n",
              " '▁ser': 370,\n",
              " 'ks': 371,\n",
              " 'ction': 372,\n",
              " 'aw': 373,\n",
              " '▁into': 374,\n",
              " '▁no': 375,\n",
              " '▁after': 376,\n",
              " '▁ear': 377,\n",
              " 'au': 378,\n",
              " '▁app': 379,\n",
              " 'reat': 380,\n",
              " 'ities': 381,\n",
              " '▁world': 382,\n",
              " 'ade': 383,\n",
              " 'ual': 384,\n",
              " '▁there': 385,\n",
              " '▁so': 386,\n",
              " 'ious': 387,\n",
              " 'olog': 388,\n",
              " '▁many': 389,\n",
              " '▁fe': 390,\n",
              " 'ach': 391,\n",
              " 'ury': 392,\n",
              " 'ose': 393,\n",
              " 'ular': 394,\n",
              " '▁under': 395,\n",
              " '▁than': 396,\n",
              " 'overn': 397,\n",
              " '▁fr': 398,\n",
              " '▁off': 399,\n",
              " '▁may': 400,\n",
              " '▁count': 401,\n",
              " '▁during': 402,\n",
              " 'gan': 403,\n",
              " '▁sch': 404,\n",
              " '▁sou': 405,\n",
              " 'ific': 406,\n",
              " 'rough': 407,\n",
              " '▁qu': 408,\n",
              " '▁state': 409,\n",
              " '▁two': 410,\n",
              " 'amp': 411,\n",
              " '▁these': 412,\n",
              " '▁govern': 413,\n",
              " '▁att': 414,\n",
              " 'ents': 415,\n",
              " 'ved': 416,\n",
              " '▁kn': 417,\n",
              " '▁inc': 418,\n",
              " 'ite': 419,\n",
              " 'ivers': 420,\n",
              " '▁acc': 421,\n",
              " 'ower': 422,\n",
              " 'ood': 423,\n",
              " '▁used': 424,\n",
              " '▁inf': 425,\n",
              " 'eric': 426,\n",
              " 'tween': 427,\n",
              " '▁between': 428,\n",
              " 'ail': 429,\n",
              " 'ased': 430,\n",
              " 'ted': 431,\n",
              " '▁sub': 432,\n",
              " 'eral': 433,\n",
              " '▁states': 434,\n",
              " 'are': 435,\n",
              " '▁par': 436,\n",
              " 'ative': 437,\n",
              " 'ase': 438,\n",
              " 'ten': 439,\n",
              " 'ild': 440,\n",
              " 'ments': 441,\n",
              " 'ok': 442,\n",
              " '▁mon': 443,\n",
              " '▁out': 444,\n",
              " '▁ac': 445,\n",
              " '▁when': 446,\n",
              " 'te': 447,\n",
              " 'low': 448,\n",
              " '▁gr': 449,\n",
              " '▁time': 450,\n",
              " 'stem': 451,\n",
              " 'les': 452,\n",
              " 'ings': 453,\n",
              " 'velop': 454,\n",
              " 'wn': 455,\n",
              " 'ress': 456,\n",
              " '▁only': 457,\n",
              " '▁br': 458,\n",
              " '▁act': 459,\n",
              " 'ublic': 460,\n",
              " '▁eu': 461,\n",
              " '▁inst': 462,\n",
              " '▁i': 463,\n",
              " '▁system': 464,\n",
              " 'ution': 465,\n",
              " 'ics': 466,\n",
              " '▁develop': 467,\n",
              " '▁4': 468,\n",
              " '▁est': 469,\n",
              " '▁up': 470,\n",
              " '▁num': 471,\n",
              " 'll': 472,\n",
              " 'ident': 473,\n",
              " '▁americ': 474,\n",
              " 'ons': 475,\n",
              " 'cial': 476,\n",
              " 'ines': 477,\n",
              " '▁trans': 478,\n",
              " 'ath': 479,\n",
              " 'iet': 480,\n",
              " '▁we': 481,\n",
              " '▁rep': 482,\n",
              " '▁car': 483,\n",
              " '▁how': 484,\n",
              " 'ool': 485,\n",
              " '▁5': 486,\n",
              " '▁bl': 487,\n",
              " '▁century': 488,\n",
              " 'ind': 489,\n",
              " '▁produ': 490,\n",
              " '▁united': 491,\n",
              " '▁def': 492,\n",
              " '▁eng': 493,\n",
              " '▁jo': 494,\n",
              " 'ince': 495,\n",
              " '▁ass': 496,\n",
              " '▁mus': 497,\n",
              " '▁inter': 498,\n",
              " '▁through': 499,\n",
              " 'ph': 500,\n",
              " '▁inv': 501,\n",
              " '▁arm': 502,\n",
              " '▁char': 503,\n",
              " '▁nor': 504,\n",
              " '▁would': 505,\n",
              " 'uc': 506,\n",
              " '▁univers': 507,\n",
              " 'old': 508,\n",
              " '▁euro': 509,\n",
              " '▁stud': 510,\n",
              " 'round': 511,\n",
              " '▁peop': 512,\n",
              " 'ond': 513,\n",
              " 'ral': 514,\n",
              " '▁co': 515,\n",
              " '▁elect': 516,\n",
              " 'iss': 517,\n",
              " '▁ap': 518,\n",
              " '▁cap': 519,\n",
              " 'one': 520,\n",
              " '▁loc': 521,\n",
              " '▁dif': 522,\n",
              " '▁fl': 523,\n",
              " 'ists': 524,\n",
              " '▁while': 525,\n",
              " '▁them': 526,\n",
              " '▁air': 527,\n",
              " '▁bu': 528,\n",
              " '▁prov': 529,\n",
              " '▁min': 530,\n",
              " '▁maj': 531,\n",
              " '▁europe': 532,\n",
              " '▁rem': 533,\n",
              " '▁brit': 534,\n",
              " 'roup': 535,\n",
              " '▁high': 536,\n",
              " '▁supp': 537,\n",
              " '▁popul': 538,\n",
              " 'ient': 539,\n",
              " '▁soc': 540,\n",
              " 'als': 541,\n",
              " 'ever': 542,\n",
              " '▁ver': 543,\n",
              " 'ating': 544,\n",
              " '▁do': 545,\n",
              " '▁major': 546,\n",
              " 'ep': 547,\n",
              " '▁use': 548,\n",
              " '▁found': 549,\n",
              " '▁people': 550,\n",
              " '▁exp': 551,\n",
              " '▁ed': 552,\n",
              " '▁government': 553,\n",
              " '▁known': 554,\n",
              " '▁years': 555,\n",
              " '▁including': 556,\n",
              " 'ural': 557,\n",
              " 'ily': 558,\n",
              " '▁group': 559,\n",
              " 'ters': 560,\n",
              " '▁mu': 561,\n",
              " '▁po': 562,\n",
              " 'form': 563,\n",
              " 'ike': 564,\n",
              " 'illion': 565,\n",
              " '▁med': 566,\n",
              " 'ished': 567,\n",
              " '▁dist': 568,\n",
              " '▁const': 569,\n",
              " 'cept': 570,\n",
              " '▁call': 571,\n",
              " '▁north': 572,\n",
              " '▁school': 573,\n",
              " '▁play': 574,\n",
              " '▁z': 575,\n",
              " '▁her': 576,\n",
              " 'tain': 577,\n",
              " 'ians': 578,\n",
              " 'ject': 579,\n",
              " '▁var': 580,\n",
              " 'ually': 581,\n",
              " 'gest': 582,\n",
              " '▁hist': 583,\n",
              " '▁about': 584,\n",
              " 'uss': 585,\n",
              " '▁dec': 586,\n",
              " 'ake': 587,\n",
              " '▁both': 588,\n",
              " '▁end': 589,\n",
              " '▁however': 590,\n",
              " 'rib': 591,\n",
              " '▁germ': 592,\n",
              " '▁well': 593,\n",
              " 'hip': 594,\n",
              " 'ures': 595,\n",
              " '▁national': 596,\n",
              " 'ized': 597,\n",
              " '▁mem': 598,\n",
              " 'io': 599,\n",
              " '▁work': 600,\n",
              " 'urn': 601,\n",
              " 'gr': 602,\n",
              " '▁fol': 603,\n",
              " '▁6': 604,\n",
              " 'gy': 605,\n",
              " 'ug': 606,\n",
              " 'any': 607,\n",
              " 'ne': 608,\n",
              " 'ys': 609,\n",
              " '▁dep': 610,\n",
              " 'ict': 611,\n",
              " '▁main': 612,\n",
              " '▁south': 613,\n",
              " '▁became': 614,\n",
              " 'angu': 615,\n",
              " '▁where': 616,\n",
              " '▁early': 617,\n",
              " 'int': 618,\n",
              " '▁rom': 619,\n",
              " '▁199': 620,\n",
              " 'ically': 621,\n",
              " '▁will': 622,\n",
              " '▁art': 623,\n",
              " '▁str': 624,\n",
              " '▁sign': 625,\n",
              " 'ause': 626,\n",
              " 'cess': 627,\n",
              " 'ix': 628,\n",
              " '▁number': 629,\n",
              " 'ants': 630,\n",
              " 'inal': 631,\n",
              " '▁intern': 632,\n",
              " 'ah': 633,\n",
              " '▁power': 634,\n",
              " '▁ob': 635,\n",
              " '▁ent': 636,\n",
              " 'ward': 637,\n",
              " '▁langu': 638,\n",
              " '▁later': 639,\n",
              " '▁differ': 640,\n",
              " '▁pos': 641,\n",
              " '▁contin': 642,\n",
              " '▁cult': 643,\n",
              " 'ise': 644,\n",
              " 'ays': 645,\n",
              " '▁again': 646,\n",
              " 'hes': 647,\n",
              " '▁cor': 648,\n",
              " '▁sm': 649,\n",
              " '▁law': 650,\n",
              " '▁eff': 651,\n",
              " '▁mil': 652,\n",
              " '▁ext': 653,\n",
              " '▁imp': 654,\n",
              " 'tle': 655,\n",
              " '▁spec': 656,\n",
              " '▁made': 657,\n",
              " '▁hum': 658,\n",
              " '▁7': 659,\n",
              " '▁add': 660,\n",
              " 'iod': 661,\n",
              " '▁follow': 662,\n",
              " '▁set': 663,\n",
              " '▁sever': 664,\n",
              " 'na': 665,\n",
              " 'lish': 666,\n",
              " '▁17': 667,\n",
              " 'cent': 668,\n",
              " '▁king': 669,\n",
              " '▁sim': 670,\n",
              " '▁large': 671,\n",
              " '▁disc': 672,\n",
              " '000': 673,\n",
              " '▁bro': 674,\n",
              " '▁since': 675,\n",
              " 'ft': 676,\n",
              " 'ri': 677,\n",
              " '▁sur': 678,\n",
              " '▁term': 679,\n",
              " 'ning': 680,\n",
              " 'ale': 681,\n",
              " '▁population': 682,\n",
              " 'sel': 683,\n",
              " 'cond': 684,\n",
              " '▁serv': 685,\n",
              " 'ered': 686,\n",
              " 'ature': 687,\n",
              " 'enc': 688,\n",
              " 'ina': 689,\n",
              " '▁lead': 690,\n",
              " '▁commun': 691,\n",
              " '▁offic': 692,\n",
              " '▁called': 693,\n",
              " '▁rev': 694,\n",
              " '▁public': 695,\n",
              " '▁pers': 696,\n",
              " 'its': 697,\n",
              " '▁trad': 698,\n",
              " '▁organ': 699,\n",
              " '▁often': 700,\n",
              " '▁second': 701,\n",
              " '▁being': 702,\n",
              " '▁contro': 703,\n",
              " 'ton': 704,\n",
              " 'imes': 705,\n",
              " '▁incre': 706,\n",
              " '▁period': 707,\n",
              " '▁gree': 708,\n",
              " '▁record': 709,\n",
              " '▁three': 710,\n",
              " 'ology': 711,\n",
              " 'til': 712,\n",
              " '▁met': 713,\n",
              " '▁chur': 714,\n",
              " '▁west': 715,\n",
              " '▁prot': 716,\n",
              " '▁8': 717,\n",
              " '▁university': 718,\n",
              " 'ible': 719,\n",
              " 'row': 720,\n",
              " 'fore': 721,\n",
              " '▁oper': 722,\n",
              " 'ros': 723,\n",
              " '▁orig': 724,\n",
              " '▁common': 725,\n",
              " '▁any': 726,\n",
              " 'other': 727,\n",
              " 'up': 728,\n",
              " '▁port': 729,\n",
              " '▁great': 730,\n",
              " '▁polit': 731,\n",
              " 'ived': 732,\n",
              " '▁music': 733,\n",
              " '▁gre': 734,\n",
              " '▁cre': 735,\n",
              " 'led': 736,\n",
              " '▁million': 737,\n",
              " '▁class': 738,\n",
              " 'ages': 739,\n",
              " '▁av': 740,\n",
              " 'af': 741,\n",
              " 'ium': 742,\n",
              " 'ange': 743,\n",
              " '▁occ': 744,\n",
              " '▁fam': 745,\n",
              " 'ork': 746,\n",
              " 'though': 747,\n",
              " '▁need': 748,\n",
              " '▁mid': 749,\n",
              " '▁land': 750,\n",
              " 'yp': 751,\n",
              " '▁emp': 752,\n",
              " '▁long': 753,\n",
              " '▁fil': 754,\n",
              " 'iel': 755,\n",
              " '▁cour': 756,\n",
              " 'chn': 757,\n",
              " '▁15': 758,\n",
              " '▁several': 759,\n",
              " 'oy': 760,\n",
              " 'min': 761,\n",
              " '▁partic': 762,\n",
              " 'hn': 763,\n",
              " 'bers': 764,\n",
              " 'ility': 765,\n",
              " 'lished': 766,\n",
              " 'ision': 767,\n",
              " 'ird': 768,\n",
              " '▁result': 769,\n",
              " 'ank': 770,\n",
              " 'conom': 771,\n",
              " 'li': 772,\n",
              " '▁dem': 773,\n",
              " '▁area': 774,\n",
              " 'az': 775,\n",
              " 'rop': 776,\n",
              " 'ital': 777,\n",
              " '▁british': 778,\n",
              " '▁estab': 779,\n",
              " '▁198': 780,\n",
              " 'ues': 781,\n",
              " '▁chr': 782,\n",
              " 'ather': 783,\n",
              " '▁american': 784,\n",
              " '▁bel': 785,\n",
              " '▁sl': 786,\n",
              " '▁moder': 787,\n",
              " 'ann': 788,\n",
              " '▁she': 789,\n",
              " '▁gl': 790,\n",
              " 'str': 791,\n",
              " '▁12': 792,\n",
              " '▁like': 793,\n",
              " '▁consid': 794,\n",
              " '▁dire': 795,\n",
              " '▁econom': 796,\n",
              " '▁je': 797,\n",
              " '▁dr': 798,\n",
              " 'arch': 799,\n",
              " '▁leg': 800,\n",
              " 'pect': 801,\n",
              " 'ick': 802,\n",
              " '▁16': 803,\n",
              " 'lu': 804,\n",
              " 'rench': 805,\n",
              " 'acter': 806,\n",
              " '▁own': 807,\n",
              " '▁ann': 808,\n",
              " '▁phil': 809,\n",
              " '▁then': 810,\n",
              " '▁against': 811,\n",
              " '▁john': 812,\n",
              " 'rent': 813,\n",
              " 'ains': 814,\n",
              " 'ins': 815,\n",
              " 'ouse': 816,\n",
              " '▁island': 817,\n",
              " 'ful': 818,\n",
              " 'ices': 819,\n",
              " '▁because': 820,\n",
              " '▁import': 821,\n",
              " '▁gu': 822,\n",
              " 'ason': 823,\n",
              " '▁among': 824,\n",
              " '▁9': 825,\n",
              " 'ames': 826,\n",
              " '▁region': 827,\n",
              " 'ued': 828,\n",
              " '▁each': 829,\n",
              " 'ars': 830,\n",
              " '▁modern': 831,\n",
              " '▁10': 832,\n",
              " '▁197': 833,\n",
              " '▁mat': 834,\n",
              " '▁country': 835,\n",
              " '▁although': 836,\n",
              " '▁small': 837,\n",
              " '▁mark': 838,\n",
              " '▁french': 839,\n",
              " '▁led': 840,\n",
              " '▁writ': 841,\n",
              " '▁conf': 842,\n",
              " 'ared': 843,\n",
              " '▁four': 844,\n",
              " 'ided': 845,\n",
              " 'ives': 846,\n",
              " '▁those': 847,\n",
              " '▁include': 848,\n",
              " '▁lit': 849,\n",
              " '▁until': 850,\n",
              " '▁east': 851,\n",
              " '▁fin': 852,\n",
              " '▁support': 853,\n",
              " '▁him': 854,\n",
              " '▁milit': 855,\n",
              " '▁techn': 856,\n",
              " '▁cur': 857,\n",
              " '▁could': 858,\n",
              " 'omin': 859,\n",
              " '▁different': 860,\n",
              " '▁sw': 861,\n",
              " 'estern': 862,\n",
              " '▁op': 863,\n",
              " '▁ge': 864,\n",
              " '▁relig': 865,\n",
              " 'ruct': 866,\n",
              " 'ute': 867,\n",
              " 'ck': 868,\n",
              " '▁cr': 869,\n",
              " '▁general': 870,\n",
              " '▁stand': 871,\n",
              " '▁ret': 872,\n",
              " '▁ter': 873,\n",
              " 'ife': 874,\n",
              " 'ained': 875,\n",
              " 'man': 876,\n",
              " '▁arch': 877,\n",
              " 'ty': 878,\n",
              " '▁church': 879,\n",
              " '▁though': 880,\n",
              " 'por': 881,\n",
              " '▁val': 882,\n",
              " '▁div': 883,\n",
              " '▁empire': 884,\n",
              " '▁christ': 885,\n",
              " '▁within': 886,\n",
              " '▁comple': 887,\n",
              " 'ger': 888,\n",
              " '▁ref': 889,\n",
              " '▁sec': 890,\n",
              " 'iew': 891,\n",
              " '▁began': 892,\n",
              " '▁influ': 893,\n",
              " '▁around': 894,\n",
              " '▁if': 895,\n",
              " '▁la': 896,\n",
              " '▁ir': 897,\n",
              " '▁human': 898,\n",
              " '▁same': 899,\n",
              " 'olution': 900,\n",
              " 'day': 901,\n",
              " 'ined': 902,\n",
              " 'aking': 903,\n",
              " 'ting': 904,\n",
              " '▁international': 905,\n",
              " 'air': 906,\n",
              " 'ries': 907,\n",
              " '▁near': 908,\n",
              " '▁fre': 909,\n",
              " 'ised': 910,\n",
              " '▁ever': 911,\n",
              " 'ross': 912,\n",
              " 'vent': 913,\n",
              " 'ified': 914,\n",
              " 'ccess': 915,\n",
              " '▁build': 916,\n",
              " '▁largest': 917,\n",
              " 'osed': 918,\n",
              " '▁hig': 919,\n",
              " '▁196': 920,\n",
              " '▁political': 921,\n",
              " '0.': 922,\n",
              " '▁design': 923,\n",
              " '▁gener': 924,\n",
              " '▁direct': 925,\n",
              " 'lev': 926,\n",
              " '▁old': 927,\n",
              " 'ob': 928,\n",
              " 'ield': 929,\n",
              " '▁even': 930,\n",
              " '▁ant': 931,\n",
              " 'ording': 932,\n",
              " 'ene': 933,\n",
              " 'ior': 934,\n",
              " '▁pat': 935,\n",
              " '▁language': 936,\n",
              " '▁lib': 937,\n",
              " '▁before': 938,\n",
              " '▁allow': 939,\n",
              " 'ample': 940,\n",
              " '▁el': 941,\n",
              " 'itation': 942,\n",
              " 'inc': 943,\n",
              " '▁much': 944,\n",
              " '▁afric': 945,\n",
              " 'ms': 946,\n",
              " 'aph': 947,\n",
              " '▁official': 948,\n",
              " '▁requ': 949,\n",
              " '▁es': 950,\n",
              " 'ience': 951,\n",
              " 'ov': 952,\n",
              " '▁success': 953,\n",
              " 'ired': 954,\n",
              " '▁sing': 955,\n",
              " 'hen': 956,\n",
              " '▁194': 957,\n",
              " 'uth': 958,\n",
              " 'way': 959,\n",
              " '▁military': 960,\n",
              " '▁german': 961,\n",
              " '▁development': 962,\n",
              " '▁non': 963,\n",
              " '▁did': 964,\n",
              " '▁example': 965,\n",
              " '▁now': 966,\n",
              " '▁house': 967,\n",
              " '▁tradition': 968,\n",
              " '▁indu': 969,\n",
              " '▁eas': 970,\n",
              " '▁president': 971,\n",
              " 'ble': 972,\n",
              " '▁social': 973,\n",
              " 'mber': 974,\n",
              " 'imate': 975,\n",
              " 'cer': 976,\n",
              " 'me': 977,\n",
              " 'yn': 978,\n",
              " '▁tem': 979,\n",
              " 'ides': 980,\n",
              " '▁order': 981,\n",
              " '▁bat': 982,\n",
              " 'ences': 983,\n",
              " '▁prom': 984,\n",
              " '▁fed': 985,\n",
              " 'atic': 986,\n",
              " '▁army': 987,\n",
              " '▁er': 988,\n",
              " '▁som': 989,\n",
              " 'ization': 990,\n",
              " '▁roman': 991,\n",
              " '▁european': 992,\n",
              " '▁pass': 993,\n",
              " '▁histor': 994,\n",
              " '▁needed': 995,\n",
              " '▁jew': 996,\n",
              " 'get': 997,\n",
              " 'xt': 998,\n",
              " '▁down': 999,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiLG-E14TPml"
      },
      "source": [
        "maxlen=len(max(cleaned_list,key=len))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQC4XYBw1QQD"
      },
      "source": [
        "def get_dataset(token_list):\n",
        "  inp_token=[i[:-1] for i in token_list]\n",
        "  target_token=[i[1:] for i in token_list]\n",
        "  ragged_inp=tf.ragged.constant(inp_token)\n",
        "  ragged_target=tf.ragged.constant(target_token)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((ragged_inp,ragged_target))\n",
        "  dataset = dataset.batch(64)\n",
        "  dataset = dataset.map(lambda x,y: ((x.to_tensor(default_value=0, shape=[None, maxlen])), y.to_tensor(default_value=0, shape=[None, maxlen])), num_parallel_calls=5)\n",
        "  dataset = dataset.prefetch(1)\n",
        "  return dataset\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huwTY3vXNWuf"
      },
      "source": [
        "dataset=get_dataset(cleaned_list)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MLTf_D4NWl7"
      },
      "source": [
        "def create_model():\n",
        "  embed_dim=300\n",
        "  vocab_size=6000\n",
        "  model=Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_size+1,output_dim=300,mask_zero=True))\n",
        "  model.add(Bidirectional(LSTM(units=128,return_sequences=True)))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "  model.add(Bidirectional(LSTM(units=128,return_sequences=True)))\n",
        "  model.add(Dropout(rate=0.1))\n",
        "  model.add(Dense(units=vocab_size+1,activation='softmax'))\n",
        "  model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics='accuracy')\n",
        "  return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY0pccdJVjOL"
      },
      "source": [
        "model=create_model()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtVmAVdFR65x",
        "outputId": "d73512f7-036e-4ed0-c90a-bba3eb3f3bac"
      },
      "source": [
        "hitory=model.fit(dataset,epochs=50,verbose=1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "372/372 [==============================] - 91s 195ms/step - loss: 1.4760 - accuracy: 0.0824\n",
            "Epoch 2/50\n",
            "372/372 [==============================] - 74s 198ms/step - loss: 1.0281 - accuracy: 0.2324\n",
            "Epoch 3/50\n",
            "372/372 [==============================] - 74s 198ms/step - loss: 0.7554 - accuracy: 0.4010\n",
            "Epoch 4/50\n",
            "372/372 [==============================] - 73s 197ms/step - loss: 0.5932 - accuracy: 0.5156\n",
            "Epoch 5/50\n",
            "372/372 [==============================] - 74s 198ms/step - loss: 0.4624 - accuracy: 0.6270\n",
            "Epoch 6/50\n",
            "372/372 [==============================] - 73s 197ms/step - loss: 0.3647 - accuracy: 0.7053\n",
            "Epoch 7/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.2947 - accuracy: 0.7598\n",
            "Epoch 8/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.2411 - accuracy: 0.8023\n",
            "Epoch 9/50\n",
            "372/372 [==============================] - 73s 197ms/step - loss: 0.1968 - accuracy: 0.8389\n",
            "Epoch 10/50\n",
            "372/372 [==============================] - 73s 197ms/step - loss: 0.1594 - accuracy: 0.8716\n",
            "Epoch 11/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.1286 - accuracy: 0.8988\n",
            "Epoch 12/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.1040 - accuracy: 0.9211\n",
            "Epoch 13/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0847 - accuracy: 0.9382\n",
            "Epoch 14/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0689 - accuracy: 0.9525\n",
            "Epoch 15/50\n",
            "372/372 [==============================] - 73s 197ms/step - loss: 0.0563 - accuracy: 0.9634\n",
            "Epoch 16/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0462 - accuracy: 0.9715\n",
            "Epoch 17/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0382 - accuracy: 0.9776\n",
            "Epoch 18/50\n",
            "372/372 [==============================] - 73s 195ms/step - loss: 0.0315 - accuracy: 0.9825\n",
            "Epoch 19/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0262 - accuracy: 0.9863\n",
            "Epoch 20/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0219 - accuracy: 0.9889\n",
            "Epoch 21/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0183 - accuracy: 0.9912\n",
            "Epoch 22/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0154 - accuracy: 0.9927\n",
            "Epoch 23/50\n",
            "372/372 [==============================] - 73s 195ms/step - loss: 0.0130 - accuracy: 0.9942\n",
            "Epoch 24/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0111 - accuracy: 0.9952\n",
            "Epoch 25/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0095 - accuracy: 0.9961\n",
            "Epoch 26/50\n",
            "372/372 [==============================] - 73s 195ms/step - loss: 0.0082 - accuracy: 0.9967\n",
            "Epoch 27/50\n",
            "372/372 [==============================] - 73s 195ms/step - loss: 0.0070 - accuracy: 0.9973\n",
            "Epoch 28/50\n",
            "372/372 [==============================] - 72s 195ms/step - loss: 0.0061 - accuracy: 0.9977\n",
            "Epoch 29/50\n",
            "372/372 [==============================] - 73s 195ms/step - loss: 0.0053 - accuracy: 0.9980\n",
            "Epoch 30/50\n",
            "372/372 [==============================] - 73s 195ms/step - loss: 0.0047 - accuracy: 0.9983\n",
            "Epoch 31/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0041 - accuracy: 0.9986\n",
            "Epoch 32/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0037 - accuracy: 0.9987\n",
            "Epoch 33/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0032 - accuracy: 0.9989\n",
            "Epoch 34/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0029 - accuracy: 0.9990\n",
            "Epoch 35/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0026 - accuracy: 0.9991\n",
            "Epoch 36/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0024 - accuracy: 0.9992\n",
            "Epoch 37/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0021 - accuracy: 0.9993\n",
            "Epoch 38/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0019 - accuracy: 0.9994\n",
            "Epoch 39/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0017 - accuracy: 0.9994\n",
            "Epoch 40/50\n",
            "372/372 [==============================] - 73s 195ms/step - loss: 0.0016 - accuracy: 0.9995\n",
            "Epoch 41/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0014 - accuracy: 0.9996\n",
            "Epoch 42/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0013 - accuracy: 0.9996\n",
            "Epoch 43/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0012 - accuracy: 0.9996\n",
            "Epoch 44/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 0.0011 - accuracy: 0.9996\n",
            "Epoch 45/50\n",
            "372/372 [==============================] - 73s 197ms/step - loss: 0.0011 - accuracy: 0.9996\n",
            "Epoch 46/50\n",
            "372/372 [==============================] - 73s 197ms/step - loss: 0.0010 - accuracy: 0.9996\n",
            "Epoch 47/50\n",
            "372/372 [==============================] - 73s 197ms/step - loss: 9.1004e-04 - accuracy: 0.9997\n",
            "Epoch 48/50\n",
            "372/372 [==============================] - 73s 197ms/step - loss: 8.5361e-04 - accuracy: 0.9997\n",
            "Epoch 49/50\n",
            "372/372 [==============================] - 73s 197ms/step - loss: 8.0307e-04 - accuracy: 0.9997\n",
            "Epoch 50/50\n",
            "372/372 [==============================] - 73s 196ms/step - loss: 7.5895e-04 - accuracy: 0.9997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "F7qNga6Ns60b",
        "outputId": "a61c38b4-e883-43d7-df70-b01f41516403"
      },
      "source": [
        "df.question.iloc[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'When did Beyonce start becoming popular?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "NneM3LpjqP04",
        "outputId": "d91de543-dd84-4a14-cc5a-7ce96bfe588d"
      },
      "source": [
        "list_of_sentence[5]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'beyonce took a hiatus from music in 2010 and took over management of her career her fourth album 4 2011 was subsequently mellower in tone exploring 1970s funk 1980s pop and 1990s soul.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP0YTSmLuQe1"
      },
      "source": [
        "test_input=texts_to_sequences(encode_data(list_of_sentence[5]))\n",
        "test_input.append(2)\n",
        "test_input=[1]+test_input"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24MDaivSpGox"
      },
      "source": [
        "test_1=test_input[:10]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDz6-tBwmnEv"
      },
      "source": [
        "out=model.predict(test_input)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ9tkrOOpwP_",
        "outputId": "ec1b6515-6777-4590-fc20-80a64b5c3fe6"
      },
      "source": [
        "out.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29, 1, 6001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvwX0QAgnGHq",
        "outputId": "0d086096-a407-4cd1-8476-4b00c3df569f"
      },
      "source": [
        "for i in range(out.shape[0]):\n",
        "  print(idx2word[np.argmax(out[i,:,:])])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "</s>\n",
            "ount\n",
            "▁well\n",
            "▁immed\n",
            "</s>\n",
            "</s>\n",
            "</s>\n",
            "</s>\n",
            "▁except\n",
            "</s>\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n",
            "itals\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPly72wqufr5"
      },
      "source": [
        "for i in range(20):\n",
        "  out=model.predict(test_1)\n",
        "  tok=np.argmax(out[1,:,:])\n",
        "  if tok==2:\n",
        "    break\n",
        "  else:\n",
        "    test_1.append(tok)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW5jxftiqNh3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEpwXAEsmK1d",
        "outputId": "2eea5580-613d-4e07-94b7-90e99ec45b09"
      },
      "source": [
        "[idx2word[i] for i in test_1]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " '▁beyonce',\n",
              " '▁took',\n",
              " '▁a',\n",
              " '▁h',\n",
              " 'i',\n",
              " 'at',\n",
              " 'us',\n",
              " '▁from',\n",
              " '▁music',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount',\n",
              " 'ount']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jIQb0huctG9",
        "outputId": "cb11c070-3387-4984-c423-adfbe7a7af0f"
      },
      "source": [
        "use_tpu = True\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "    with strategy.scope():\n",
        "        model = create_model()\n",
        "else:\n",
        "    model = create_model()\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.68.105.50:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.68.105.50:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 300)         1800300   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 256)         439296    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, None, 256)         394240    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 6001)        1542257   \n",
            "=================================================================\n",
            "Total params: 4,176,093\n",
            "Trainable params: 4,176,093\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU5qC_b8PMvy"
      },
      "source": [
        "model.predict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1UYBrW1Qls2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}